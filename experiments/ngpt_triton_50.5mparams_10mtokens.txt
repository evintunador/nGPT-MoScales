import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import itertools
import tiktoken
import json
import math

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
torch.set_float32_matmul_precision('high')
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.attention.flex_attention import BlockMask, flex_attention, create_block_mask

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

from kernels.cos_norm import cosine_norm_triton_ as cosine_norm_inplace_
from kernels.cos_norm import cosine_norm_triton as cosine_norm
#from kernels.resid import resid_fwd_naive as resid
from kernels.resid import resid_fwd_triton as resid
#from kernels.resid import resid_fwd_frankenstein as resid # out of VRAM error bc of bwd

def norm_(x: Tensor, dim: int = -1):
    """in-place cosine normalization"""
    x.div_(x.norm(p=2, dim=dim, keepdim=True))

class Scale(nn.Module):
    """
    A module that manages learnable scaling parameters to ensure different learning rates
    from the rest of the parameters in the model (see pages 5 and 19)
    
    Args:
        dim (int): Dimension of the scaling parameter
        scale (float): Initial scale value
        init (float): Initial value for the scaling parameter
        device (str, optional): Device to store the parameter on
    """
    def __init__(self, dim: int, heads: int = 1, scale: float = 1.0, init: float = 1.0):
        super().__init__()
        self.init = init
        self.scale = scale
        self.s = nn.Parameter((torch.ones(heads, dim) if heads > 1 else torch.ones(dim)) * scale)
            # heads == 1 gives us a single regular vector
            # heads > 1 gets used in attention mechanism for different scaling vector for each head
    
    def forward(self):
        """Compute the effective scaling factor."""
        return self.s * (self.init / self.scale) # shape (heads, dim)

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # Ensure we don't exceed the dimension size
        dim_quarter = max(1, dim // 4)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim_quarter, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim_quarter)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq) # outer product
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        # Handle case where the number of dimensions is smaller
        dim_half = x_BTHD.size(-1) // 2
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos[..., :dim_half] + x2 * sin[..., :dim_half]
        y2 = x1 * (-sin[..., :dim_half]) + x2 * cos[..., :dim_half]
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=None):
        super().__init__()
        # Calculate head_dim based on model dimensions and num_heads
        self.num_heads = num_heads
        # If head_dim not specified, calculate it based on the model dimension
        self.head_dim = dim // num_heads if head_dim is None else head_dim
        self.Wq = nn.Linear(dim, num_heads * self.head_dim, bias=False)
        self.Wk = nn.Linear(dim, num_heads * self.head_dim, bias=False)
        self.Wv = nn.Linear(dim, num_heads * self.head_dim, bias=False)
        # the scaling factor to apply to the normalized queries & keys (see page 4)
        self.s_qk = Scale(self.head_dim, heads=num_heads, scale = 1. / math.sqrt(dim))
        # the scaling factor to apply to the attention logits to restore a variance of 1 (see page 4)
        self.scale = self.head_dim ** 0.5
        self.rotary = Rotary(self.head_dim, max_seq_len)
        self.Wo = nn.Linear(num_heads * self.head_dim, dim, bias=False)

    def forward(self, x: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        # Linear projections for queries, keys, and values
        q, k, v = self.Wq(x), self.Wk(x), self.Wv(x)
            # shape: (batch_size, seq_len, dim) -> (batch_size, seq_len, num_heads * head_dim)
        # Reshape projections to separate heads
        q = q.view(B, T, self.num_heads, self.head_dim)
        k = k.view(B, T, self.num_heads, self.head_dim)
        v = v.view(B, T, self.num_heads, self.head_dim)
        # normalizing & scaling our queries  & keys (see page 4)
        s_qk = self.s_qk() # (num_heads, head_dim)
        q = cosine_norm(q) * s_qk # then scale each head
        k = cosine_norm(k) * s_qk # no shape change
        # apply RoPE
        q, k = self.rotary(q), self.rotary(k)
        # the meat of the attention calculation
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=0.12).transpose(1, 2)
        # combine heads
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        # mix heads
        y = self.Wo(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int, mlp_ratio: int = 4):
        super().__init__()
        hdim = int(mlp_ratio * dim)
        self.Wup = nn.Linear(dim, hdim, bias=False)
        self.Wgate = nn.Linear(dim, hdim, bias=False)
        self.Wdown = nn.Linear(hdim, dim, bias=False)
        # this flag designates Wdown to have a different parameter initialization as defined in model.py
        self.Wdown.GPT_scale_init = 1 
        # the learnable scaling factors
        self.s_u = Scale(hdim)
        self.s_v = Scale(hdim)
        # the varaince-controlling scaling term, needed to benefit from SiLU (see appendix A.1)
        self.scale = math.sqrt(dim)

    def forward(self, x: Tensor):
        # our up & gate projections
        u = self.Wup(x) # (batch_size, seq_len, hidden_dim)
        v = self.Wgate(x)
        # scale them
        u = u * self.s_u()
        v = v * self.s_v() * self.scale 
        # now perform the nonlinearity gate
        hidden = u * F.silu(v) # (batch_size, seq_len, hidden_dim)
        return self.Wdown(hidden) # (batch_size, seq_len, output_dim)

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, mlp_ratio: int, max_seq_len: int, layer_idx: int):
        super().__init__()
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len)
        self.mlp = MLP(dim, mlp_ratio)

        self.alpha_A = Scale(dim, init = 0.05, scale = 1. / math.sqrt(dim))
            # not sure what scale to use with a_A and a_M. At one point i had it as 1./math.sqrt(cfg.dim)
            # but now i can't find the reference to that in the paper
        # eigen learning rate vector
        self.alpha_M = Scale(dim, init = 0.05, scale = 1. / math.sqrt(dim))

    def forward(self, x: Tensor, block_mask: BlockMask):
        #x_A = cosine_norm(self.attn(x, block_mask))
        #x = cosine_norm(x + self.alpha_A() * (x_A - x))
        x = resid(x, self.attn(x, block_mask), self.alpha_A())
        #x_M = cosine_norm(self.mlp(x))
        #x = cosine_norm(x + self.alpha_M() * (x_M - x))
        x = resid(x, self.mlp(x), self.alpha_M())
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int, mlp_ratio: int):
        super().__init__()
        self.max_seq_len = max_seq_len
        self.model_dim = model_dim
        self.vocab_size = vocab_size
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, mlp_ratio, max_seq_len, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. this originates from Karpathy's experiments.
        self.lm_head = nn.Linear(model_dim, next_multiple_of_n(vocab_size, n=128))
        # scaling param to un-limit the range for the final probability distribution (see page 2)
        self.s_z = Scale(next_multiple_of_n(vocab_size, n=128), scale = 1./math.sqrt(model_dim))
        # initializing params to specific distributions
        self.apply(self.__init__weights)

    def __init__weights(self, module):
        """
        parameter initialization isn't actually important in N-GPT because of the normalization
        However we'll still initialize according to how they did in appendix A.5
        """
        # whereas GPT-2 used std = 0.02, we'll do square root of model's embedding dimension
        std = math.sqrt(self.model_dim) 

        if isinstance(module, (nn.Linear, nn.Parameter)):
            # specific weight matrices at the end of each layer are given smaller std 
            # originally this was done in GPT-2 to keep the residual stream small
            if hasattr(module, 'GPT_scale_init'):
                std *= (2 * len(self.blocks)) ** -0.5

            # carries out the actual initialization
            torch.nn.init.normal_(module.weight, mean=0.0, std=std)

            # biases, if any, should instead be initialized to zeros
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias) 

        # the embedding matrix doesn't count as an nn.Linear so we've gotta do it again for that
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=std)

    def forward(self, input_seq: Tensor, target_seq: Tensor = None):
        assert input_seq.ndim == 1

        docs = (input_seq == 50256).cumsum(0)
        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask
        doc_causal_mask = create_block_mask(document_causal, B=None, H=None, Q_LEN=input_seq.size(0), KV_LEN=input_seq.size(0))
      
        x = self.embed(input_seq)[None]

        for i in range(len(self.blocks)):
            x = self.blocks[i](x, doc_causal_mask)

        logits = self.lm_head(x).float()
        # to un-limit the temperature of the final probability distribution (see page 2)
        scaled_logits = logits * self.s_z()
        
        if target_seq is None:
            return scaled_logits
        else:
            return F.cross_entropy(scaled_logits.view(-1, logits.size(-1)), target_seq, 
                                  reduction='sum' if self.training else 'mean')

    def get_num_params(self):
        """
        Return the number of parameters in the model.
        """
        n_params = sum(p.numel() for p in self.parameters())
        return n_params
    
    def normalize_linear(self, module):
        """
        Helper method to normalize Linear layer weights where one dimension matches model dim
        """
        # Find the dimension that matches cfg.dim
        dim_to_normalize = None
        for dim, size in enumerate(module.weight.shape):
            if size == self.model_dim:
                dim_to_normalize = dim
                break
        
        if dim_to_normalize is not None:
            if dim_to_normalize == module.weight.ndim - 1:
                # Normalize the weights in-place
                cosine_norm_inplace_(module.weight.data, dim=dim_to_normalize)
            else:
                norm_(module.weight.data, dim=dim_to_normalize)

    def enforce_constraints(self):
        """
        Enforces constraints after each optimization step:
        1. Absolute value constraint on eigen learning rate parameters
        2. Cosine normalization on Linear layer weights where one dimension matches model dim
        """
        with torch.no_grad():
            # Enforce absolute value on eigen learning rates
            for layer in self.blocks:
                layer.alpha_A.s.data.abs_()
                layer.alpha_M.s.data.abs_()
            
            # Cosine normalize relevant Linear layers
            for module in self.modules():
                if isinstance(module, (nn.Linear, nn.Embedding)):
                    self.normalize_linear(module)

    @torch.no_grad()
    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
        """
        Take a conditioning sequence of indices idx (LongTensor of shape (t)) and complete
        the sequence max_new_tokens times, feeding the predictions back into the model each time.
        Most likely you'll want to make sure to be in model.eval() mode of operation for this.
        """
        assert idx.ndim == 1
        def cdiv(m, n):
            return (m + (n - 1)) // n
        seq_len = idx.size(0)
        if seq_len % 128 != 0:
            pad_ct = cdiv(seq_len, 128) * 128 - seq_len
            idx = torch.cat((idx, torch.zeros(pad_ct, dtype=idx.dtype, device=idx.device)), dim=0)
        
        self.eval()  # Ensure model is in evaluation mode
        for _ in range(max_new_tokens):
            # Forward pass to get logits
            logits = self(idx[-self.max_seq_len:] if idx.size(0) > self.max_seq_len else idx)
            # Focus on the last token's prediction
            logits = logits[0, min(seq_len, self.max_seq_len) - 1, :] / temperature
            # optionally crop the logits to only the top k options
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[-1]] = -float('Inf')
            # apply softmax to convert logits to (normalized) probabilities
            probs = F.softmax(logits, dim=-1)
            # sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1)
            while idx_next >= self.vocab_size: # don't want to grab any of the out-of-vocab tokens
                idx_next = torch.multinomial(probs, num_samples=1)
                # could get stuck in an infinite loop here bit that's hella unlikely & i'm lazy
            # append sampled index to the running sequence and continue
            idx[min(seq_len, self.max_seq_len)] = idx_next

            # iterate sequence count and account for any time we surpass flex-attention's block size
            seq_len += 1
            if (seq_len - 1) % 128 == 0:
                pad_ct = cdiv(seq_len, 128) * 128 - seq_len
                idx = torch.cat((idx, [0] * pad_ct), dim=0)

        return idx[:seq_len]

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank: int, world_size: int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise ValueError(f"No files found matching pattern: {filename_pattern}")
    
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    
    # Calculate total tokens across all shards
    total_tokens = 0
    tokens_per_file = []
    for file in files:
        header = torch.from_file(str(file), False, 256, dtype=torch.int32)
        file_tokens = int(header[2])
        total_tokens += file_tokens
        tokens_per_file.append(file_tokens)
    
    # Calculate how many tokens we need for training
    tokens_needed = args.num_iterations * batch_size
    
    # Determine if we need to cycle and calculate epochs
    will_cycle = total_tokens < tokens_needed
    epochs = tokens_needed / total_tokens if total_tokens > 0 else 0
    
    if rank == 0:
        print0(f"Total tokens across {len(files)} shard(s): {total_tokens:,}", console=True)
        print0(f"Tokens needed for {args.num_iterations} iterations: {tokens_needed:,}", console=True)
        print0(f"Training will use approximately {epochs:.2f} epochs over the data", console=True)
    
    file_iter = itertools.cycle(files) if will_cycle else iter(files)
    tokens, pos = _load_data_shard(next(file_iter)), 0
    
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb*10B/fineweb*_train_*.bin" # input .bin to train on
    val_files = "data/fineweb*10B/fineweb*_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 10*1024 # FlexAttention sequence length - reduced from 48*1024 for GPUs w/ at least 8GB VRAM during testing
    val_seq_len = 16*1024 # FlexAttention sequence length for validation - reduced from 4*64*1024
    # optimization
    num_iterations = 500 # number of iterations to run
    lr_init = 0.001
    lr_final = 0.0001
    # architecture
    vocab_size = 50257
    # model size - setup for GPUs w/ 8GB of VRAM
    num_layers = 8
    num_heads = 6
    model_dim = 384
    head_dim = None  # if None, will be set to model_dim // num_heads
    mlp_ratio = int(4 * 2/3) # 2/3 to make the GLU number of parameters eqiuvalent to not GLU
    # evaluation and logging
    val_loss_every = 100 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False

    def __post_init__(self):
        # Validate and set derived parameters
        if self.head_dim is None:
            self.head_dim = self.model_dim // self.num_heads
        assert self.head_dim in [2 ** i for i in range(1, 10)], f"head_dim must be a power of 2, got {self.head_dim}"
        assert self.mlp_ratio > 0, f"mlp_ratio must be positive, got {self.mlp_ratio}"
        assert self.train_seq_len % 128 == 0, f"train_seq_len must be multiple of 128, got {self.train_seq_len}"
        assert self.val_seq_len % 128 == 0, f"val_seq_len must be multiple of 128, got {self.val_seq_len}"

args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
# Remove assertion for 8xH100
# assert world_size == 8 # this code is designed for 8xH100
print(f"Running with {world_size} GPUs (designed originally for 8xH100, adapted to also support 2x GPUs w/ at least 8GB VRAM during testing)")
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("experiments", exist_ok=True)  # Changed from "logs" to "experiments"
    logfile = f"experiments/{run_id}.txt"  # Changed from "logs/" to "experiments/"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing all relevant files
print0(code)  # Print this file's code

# Print hellaswag.py if it exists
try:
    with open("hellaswag.py", "r") as f:
        print0("\n" + "="*100 + "\nhellaswag.py:\n" + "="*100)
        print0(f.read())
except FileNotFoundError:
    print0("\n" + "="*100 + "\nhellaswag.py not found\n" + "="*100)

print0("="*100)

# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, 
                       num_layers=args.num_layers,
                       num_heads=args.num_heads, 
                       model_dim=args.model_dim,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len),
                       mlp_ratio=args.mlp_ratio).cuda()
print0(f'{model.get_num_params()} parameters', console=True)
print0(model)

for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# Simple optimizer initialization - zero weight decay
optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr_init, betas=(0.9, 0.95), eps=1e-8, weight_decay=0.0)

# Learning rate schedule without warmup
def get_lr(step: int): # TODO add warmup for regular GPT
    # Cosine decay phase only
    cosine_decay = 0.5 * (1 + math.cos(math.pi * step / args.num_iterations))
    return max(cosine_decay, args.lr_final / args.lr_init)

# Use a more memory-efficient compilation option
model: nn.Module = torch.compile(model, dynamic=False, mode="reduce-overhead")

# Add fallback mode to handle compilation errors
import torch._dynamo
torch._dynamo.config.suppress_errors = True

########################################
#            Warmup kernels            #
########################################

# Attempt to limit memory fragmentation
if hasattr(torch.cuda, 'memory_stats'):
    print0(f"Initial GPU memory: {torch.cuda.memory_allocated() // (1024 * 1024)} MB")

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizer=copy.deepcopy(optimizer.state_dict())) # save the initial state
for _ in range(warmup_steps):
    inputs = targets = torch.randint(0, args.vocab_size, size=(args.train_seq_len,), device="cuda")
    model(inputs.to(torch.int32), targets).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    optimizer.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
optimizer.load_state_dict(initial_state["optimizer"])
del initial_state

if hasattr(torch.cuda, 'memory_stats'):
    print0(f"After warmup GPU memory: {torch.cuda.memory_allocated() // (1024 * 1024)} MB")

########################################
#        Training and validation       #
########################################

def sample_from_model(model, prompt, max_new_tokens=100, temperature=0.8, top_k=200):
    """Generate text samples from the model given a prompt."""
    # We need an encoding function - assuming you'll use tiktoken or similar
    enc = tiktoken.get_encoding("gpt2")
    encode = lambda s: enc.encode(s, allowed_special={"<|endoftext|>"})
    decode = lambda l: enc.decode(l)
    
    # Encode the prompt
    input_ids = encode(prompt)
    x = torch.tensor(input_ids, dtype=torch.int32, device="cuda")
    
    # Generate
    model.eval()
    with torch.no_grad():
        y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)
    
    # Decode and return
    return decode(y.tolist())

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        
        # Use smaller val batch for GPUs w/ at least 8GB VRAM during testing
        val_batch_size = world_size * args.val_seq_len
        # Ensure we validate on enough tokens while keeping memory usage reasonable
        val_steps = max(1, min(16, args.val_tokens // val_batch_size))
        val_tokens_used = val_batch_size * val_steps
        
        print0(f"Validating on {val_tokens_used} tokens ({val_steps} steps with {val_batch_size} batch size)")
        
        # Choose between real data loader and synthetic data loader for validation
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for i in range(val_steps):
                inputs, targets = next(val_loader)
                # Check if inputs exceed sequence length
                if inputs.size(0) > args.val_seq_len:
                    inputs = inputs[:args.val_seq_len]
                    targets = targets[:args.val_seq_len]
                val_loss += model(inputs, targets)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizer=optimizer.state_dict())
            os.makedirs(f"experiments/{run_id}", exist_ok=True) # Changed from "logs/" to "experiments/"
            torch.save(log, f"experiments/{run_id}/state_step{step:06d}.pt") # Changed from "logs/" to "experiments/"
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    
    # Check if inputs exceed sequence length - can happen if the dataset has different sized examples
    if inputs.size(0) > args.train_seq_len:
        inputs = inputs[:args.train_seq_len]
        targets = targets[:args.train_seq_len]
        
    model(inputs, targets).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # set optimization hyperparameters
    for group in optimizer.param_groups:
        group["lr"] = args.lr_init * get_lr(step)
    # step the optimizers
    optimizer.step()
    # Apply cosine normalization & absolute value constraints after optimization step
    model.enforce_constraints()  
    # null the gradients
    model.zero_grad(set_to_none=True)
        
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)

# Then at the end of training:
if master_process:
    # check to make sure abs val & cos norm actually worked
    # checking to make sure the absolute value-ing worked
    print0("-"*10 + " making sure assertions worked " + "-"*10, console=True)
    print0(model.blocks[0].alpha_A.s.data[:5], console=True)
    # checking to make sure the cosine normalization worked
    print0(model.blocks[0].mlp.Wup.weight.norm(dim=1)[:5], console=True)
    print0(model.embed.weight.norm(dim=1)[:5], console=True)

    prompts = [
        "Once upon a time,",
        "The meaning of life is",
        "In the year 2026,",
        "I'm a Large Language Model (LLM), which means"
    ]
    for prompt in prompts:
        continuation = sample_from_model(model, prompt, max_new_tokens=16)
        print0(continuation, console=True)

########################################
#        HellaSwag Evaluation         #
########################################

def render_hellaswag_example(example, enc):
    """
    Given the example as a dictionary, render it as three torch tensors:
    - tokens (the tokens of context + completion, of size 4xN, as there are always 4 candidates)
    - mask (is 1 in the region of the candidate completion, where we evaluate likelihoods)
    - label (the index of the correct completion, which we hope has the highest likelihood)
    """
    ctx = example["ctx"]
    label = example["label"]
    endings = example["endings"]

    # gather up all the tokens
    ctx_tokens = enc.encode(ctx)
    tok_rows = []
    mask_rows = []
    for end in endings:
        end_tokens = enc.encode(" " + end)  # note: prepending " " because GPT-2 tokenizer
        tok_rows.append(ctx_tokens + end_tokens)
        mask_rows.append([0]*len(ctx_tokens) + [1]*len(end_tokens))

    # have to be careful during the collation because the number of tokens in each row can differ
    max_len = max(len(row) for row in tok_rows)
    tokens = torch.zeros((4, max_len), dtype=torch.int32)
    mask = torch.zeros((4, max_len), dtype=torch.int32)
    for i, (tok_row, mask_row) in enumerate(zip(tok_rows, mask_rows)):
        tokens[i, :len(tok_row)] = torch.tensor(tok_row)
        mask[i, :len(mask_row)] = torch.tensor(mask_row)

    return tokens, mask, label

def iterate_hellaswag_examples(data_path, limit=None):
    """Iterate through HellaSwag examples, with optional limit"""
    with open(data_path, "r") as f:
        for i, line in enumerate(f):
            if limit is not None and i >= limit:
                break
            example = json.loads(line)
            yield example

@torch.no_grad()
def evaluate_hellaswag(model, data_path, limit=None):
    """Evaluate model on HellaSwag"""
    print0("Starting HellaSwag evaluation...", console=True)
    
    # Add this line at the beginning of the function to disable dynamo compilation for evaluation
    torch._dynamo.config.disable = True
    
    # Set up tokenizer
    enc = tiktoken.get_encoding("gpt2")
    
    model.eval()
    
    num_correct_norm = 0
    num_correct = 0
    num_total = 0
    
    for example in iterate_hellaswag_examples(data_path, limit):
        tokens, mask, label = render_hellaswag_example(example, enc)
        tokens = tokens.to(device="cuda")
        mask = mask.to(device="cuda")

        # Process each candidate one at a time to avoid memory issues
        losses = []
        normalized_losses = []
        
        for i in range(4):  # 4 candidates per example
            # Get token sequence for this candidate
            seq = tokens[i]
            seq_mask = mask[i]
            
            # Only process up to valid tokens (not padding)
            valid_len = (seq > 0).sum().item()
            if valid_len == 0:
                continue
                
            valid_seq = seq[:valid_len]
            
            # Get logits from our model
            logits = model(valid_seq)
            if isinstance(logits, torch.Tensor):
                logits = logits[0]  # Our model returns [B, T, V] but B=1
            
            # Evaluate the autoregressive loss
            shift_logits = logits[:-1, :]
            shift_tokens = valid_seq[1:].to(torch.int64)  # Target needs to be int64
            shift_mask = seq_mask[1:valid_len]  # Shift mask to align with shifted tokens
            
            # Calculate loss for each position
            losses_per_token = F.cross_entropy(
                shift_logits, shift_tokens, reduction='none'
            )
            
            # Apply mask to focus on completion region
            masked_losses = losses_per_token * shift_mask
            
            # Calculate total and normalized loss
            total_loss = masked_losses.sum()
            completion_token_count = shift_mask.sum()
            normalized_loss = total_loss / completion_token_count if completion_token_count > 0 else float('inf')
            
            losses.append(total_loss.item())
            normalized_losses.append(normalized_loss.item())
        
        # Get predictions
        pred = torch.tensor(losses).argmin().item()
        pred_norm = torch.tensor(normalized_losses).argmin().item()
        
        # Accumulate stats
        num_total += 1
        num_correct += int(pred == label)
        num_correct_norm += int(pred_norm == label)
        
        if num_total <= 5:  # Show details for first few examples
            print0(f"---\nContext:\n {example['ctx']}\nEndings:", console=True)
            for i, end in enumerate(example["endings"]):
                print0(f"{i} (loss: {normalized_losses[i]:.4f}) {end}", console=True)
            print0(f"predicted: {pred_norm}, actual: {label}", console=True)
    
    # Calculate accuracy
    accuracy = num_correct / num_total if num_total > 0 else 0
    accuracy_norm = num_correct_norm / num_total if num_total > 0 else 0
    
    # Calculate 95% confidence intervals using Wilson score interval
    # This is more robust than normal approximation, especially for small sample sizes or extreme probabilities
    z = 1.96  # 95% confidence
    
    def wilson_conf_interval(correct, total):
        """Calculate Wilson score interval for a binary proportion"""
        if total == 0:
            return (0, 0)
        
        p = correct / total
        denominator = 1 + z**2 / total
        centre_adjusted_p = (p + z**2 / (2 * total)) / denominator
        adjusted_interval = z * ((p * (1 - p) / total + z**2 / (4 * total**2)) ** 0.5) / denominator
        
        lower = max(0, centre_adjusted_p - adjusted_interval)
        upper = min(1, centre_adjusted_p + adjusted_interval)
        
        return (lower, upper)
    
    # Get confidence intervals
    ci = wilson_conf_interval(num_correct, num_total)
    ci_norm = wilson_conf_interval(num_correct_norm, num_total)
    
    # Final results
    print0(f"HellaSwag evaluation complete - {num_total} examples", console=True)
    print0(f"Accuracy: {num_correct}/{num_total}={accuracy:.4f} [95% CI: {ci[0]:.4f}-{ci[1]:.4f}]", console=True)
    print0(f"Normalized accuracy: {num_correct_norm}/{num_total}={accuracy_norm:.4f} [95% CI: {ci_norm[0]:.4f}-{ci_norm[1]:.4f}]", console=True)

# After training and sample generations, evaluate on HellaSwag
if master_process:
    hellaswag_path = "./data/hellaswag/hellaswag_val.jsonl"  # Adjust path as needed
    
    # Check if the HellaSwag data file exists
    if os.path.exists(hellaswag_path):
        print0(f"Found HellaSwag dataset at {hellaswag_path}, running evaluation...", console=True)
        evaluate_hellaswag(model, hellaswag_path, limit=1014)
    else:
        print0(f"HellaSwag dataset not found at {hellaswag_path}, skipping evaluation.", console=True)

dist.destroy_process_group()

====================================================================================================
hellaswag.py not found
====================================================================================================
====================================================================================================
Running Python 3.11.10 | packaged by conda-forge | (main, Oct 16 2024, 01:27:36) [GCC 13.3.0]
Running PyTorch 2.6.0+cu124 compiled for CUDA 12.4
Tue Apr  1 19:28:34 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.86.16              Driver Version: 570.86.16      CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3060        On  |   00000000:01:00.0 Off |                  N/A |
|  0%   43C    P0             41W /  170W |     347MiB /  12288MiB |      2%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA GeForce RTX 3060        On  |   00000000:81:00.0 Off |                  N/A |
|  0%   39C    P0             25W /  170W |     222MiB /  12288MiB |      2%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
50534016 parameters
GPT(
  (embed): Embedding(50257, 384)
  (blocks): ModuleList(
    (0-7): 8 x Block(
      (attn): CausalSelfAttention(
        (Wq): Linear(in_features=384, out_features=384, bias=False)
        (Wk): Linear(in_features=384, out_features=384, bias=False)
        (Wv): Linear(in_features=384, out_features=384, bias=False)
        (s_qk): Scale()
        (rotary): Rotary()
        (Wo): Linear(in_features=384, out_features=384, bias=False)
      )
      (mlp): MLP(
        (Wup): Linear(in_features=384, out_features=768, bias=False)
        (Wgate): Linear(in_features=384, out_features=768, bias=False)
        (Wdown): Linear(in_features=768, out_features=384, bias=False)
        (s_u): Scale()
        (s_v): Scale()
      )
      (alpha_A): Scale()
      (alpha_M): Scale()
    )
  )
  (lm_head): Linear(in_features=384, out_features=50304, bias=True)
  (s_z): Scale()
)
Initial GPU memory: 224 MB
After warmup GPU memory: 224 MB
Validating on 524288 tokens (16 steps with 32768 batch size)
Total tokens across 1 shard(s): 100,000,000
Tokens needed for 500 iterations: 16,384,000
Training will use approximately 0.16 epochs over the data
step:0/500 val_loss:82.9413 train_time:0ms step_avg:0.28ms
Total tokens across 5 shard(s): 500,000,000
Tokens needed for 500 iterations: 10,240,000
Training will use approximately 0.02 epochs over the data
step:1/500 train_time:926ms step_avg:926.19ms
step:2/500 train_time:940ms step_avg:470.08ms
step:3/500 train_time:952ms step_avg:317.41ms
step:4/500 train_time:964ms step_avg:241.06ms
step:5/500 train_time:1219ms step_avg:243.74ms
step:6/500 train_time:1767ms step_avg:294.44ms
step:7/500 train_time:2298ms step_avg:328.30ms
step:8/500 train_time:2760ms step_avg:345.06ms
step:9/500 train_time:3220ms step_avg:357.82ms
step:10/500 train_time:3684ms step_avg:368.45ms
step:11/500 train_time:4343ms step_avg:394.80ms
step:12/500 train_time:5238ms step_avg:436.52ms
step:13/500 train_time:5879ms step_avg:452.22ms
step:14/500 train_time:6402ms step_avg:457.25ms
step:15/500 train_time:6915ms step_avg:460.98ms
step:16/500 train_time:7422ms step_avg:463.88ms
step:17/500 train_time:7873ms step_avg:463.15ms
step:18/500 train_time:8526ms step_avg:473.65ms
step:19/500 train_time:8991ms step_avg:473.19ms
step:20/500 train_time:9546ms step_avg:477.28ms
step:21/500 train_time:10082ms step_avg:480.12ms
step:22/500 train_time:10603ms step_avg:481.95ms
step:23/500 train_time:11094ms step_avg:482.35ms
step:24/500 train_time:11562ms step_avg:481.73ms
step:25/500 train_time:12079ms step_avg:483.17ms
step:26/500 train_time:12570ms step_avg:483.48ms
step:27/500 train_time:13060ms step_avg:483.72ms
step:28/500 train_time:13594ms step_avg:485.50ms
step:29/500 train_time:14072ms step_avg:485.24ms
step:30/500 train_time:14538ms step_avg:484.60ms
step:31/500 train_time:15074ms step_avg:486.26ms
step:32/500 train_time:15810ms step_avg:494.07ms
step:33/500 train_time:16323ms step_avg:494.65ms
step:34/500 train_time:16877ms step_avg:496.38ms
step:35/500 train_time:17366ms step_avg:496.17ms
step:36/500 train_time:17865ms step_avg:496.26ms
step:37/500 train_time:18483ms step_avg:499.54ms
step:38/500 train_time:19100ms step_avg:502.64ms
step:39/500 train_time:19656ms step_avg:503.99ms
step:40/500 train_time:20181ms step_avg:504.53ms
step:41/500 train_time:20645ms step_avg:503.53ms
step:42/500 train_time:21182ms step_avg:504.33ms
step:43/500 train_time:21685ms step_avg:504.30ms
step:44/500 train_time:22680ms step_avg:515.47ms
step:45/500 train_time:23292ms step_avg:517.61ms
step:46/500 train_time:23953ms step_avg:520.71ms
step:47/500 train_time:24547ms step_avg:522.27ms
step:48/500 train_time:25001ms step_avg:520.85ms
step:49/500 train_time:25916ms step_avg:528.90ms
step:50/500 train_time:26658ms step_avg:533.15ms
step:51/500 train_time:27328ms step_avg:535.85ms
step:52/500 train_time:27961ms step_avg:537.72ms
step:53/500 train_time:28487ms step_avg:537.50ms
step:54/500 train_time:28972ms step_avg:536.51ms
step:55/500 train_time:29439ms step_avg:535.26ms
step:56/500 train_time:29882ms step_avg:533.62ms
step:57/500 train_time:30395ms step_avg:533.24ms
step:58/500 train_time:30886ms step_avg:532.52ms
step:59/500 train_time:31396ms step_avg:532.13ms
step:60/500 train_time:31912ms step_avg:531.87ms
step:61/500 train_time:32404ms step_avg:531.22ms
step:62/500 train_time:32925ms step_avg:531.05ms
step:63/500 train_time:33463ms step_avg:531.16ms
step:64/500 train_time:34158ms step_avg:533.73ms
step:65/500 train_time:34797ms step_avg:535.33ms
step:66/500 train_time:35370ms step_avg:535.91ms
step:67/500 train_time:35907ms step_avg:535.92ms
step:68/500 train_time:36421ms step_avg:535.61ms
step:69/500 train_time:36898ms step_avg:534.75ms
step:70/500 train_time:37445ms step_avg:534.93ms
step:71/500 train_time:37984ms step_avg:534.99ms
step:72/500 train_time:38713ms step_avg:537.68ms
step:73/500 train_time:39211ms step_avg:537.13ms
step:74/500 train_time:39663ms step_avg:535.99ms
step:75/500 train_time:40281ms step_avg:537.08ms
step:76/500 train_time:40824ms step_avg:537.16ms
step:77/500 train_time:41309ms step_avg:536.48ms
step:78/500 train_time:41787ms step_avg:535.73ms
step:79/500 train_time:42512ms step_avg:538.12ms
step:80/500 train_time:43151ms step_avg:539.38ms
step:81/500 train_time:43778ms step_avg:540.47ms
step:82/500 train_time:44312ms step_avg:540.38ms
step:83/500 train_time:44928ms step_avg:541.30ms
step:84/500 train_time:45536ms step_avg:542.09ms
step:85/500 train_time:46265ms step_avg:544.30ms
step:86/500 train_time:47093ms step_avg:547.59ms
step:87/500 train_time:47727ms step_avg:548.58ms
step:88/500 train_time:48235ms step_avg:548.13ms
step:89/500 train_time:48804ms step_avg:548.36ms
step:90/500 train_time:49282ms step_avg:547.58ms
step:91/500 train_time:49796ms step_avg:547.21ms
step:92/500 train_time:50789ms step_avg:552.06ms
step:93/500 train_time:51624ms step_avg:555.10ms
step:94/500 train_time:52268ms step_avg:556.04ms
step:95/500 train_time:52785ms step_avg:555.63ms
step:96/500 train_time:53570ms step_avg:558.02ms
step:97/500 train_time:54111ms step_avg:557.84ms
step:98/500 train_time:54577ms step_avg:556.90ms
step:99/500 train_time:55117ms step_avg:556.74ms
step:100/500 train_time:55610ms step_avg:556.10ms
Validating on 524288 tokens (16 steps with 32768 batch size)
Total tokens across 1 shard(s): 100,000,000
Tokens needed for 500 iterations: 16,384,000
Training will use approximately 0.16 epochs over the data
step:100/500 val_loss:8.6408 train_time:57647ms step_avg:576.47ms
step:101/500 train_time:57661ms step_avg:570.90ms
step:102/500 train_time:57673ms step_avg:565.42ms
step:103/500 train_time:57685ms step_avg:560.05ms
step:104/500 train_time:57956ms step_avg:557.27ms
step:105/500 train_time:58570ms step_avg:557.81ms
step:106/500 train_time:59031ms step_avg:556.89ms
step:107/500 train_time:59523ms step_avg:556.29ms
step:108/500 train_time:60001ms step_avg:555.56ms
step:109/500 train_time:60527ms step_avg:555.29ms
step:110/500 train_time:61086ms step_avg:555.32ms
step:111/500 train_time:61545ms step_avg:554.46ms
step:112/500 train_time:62030ms step_avg:553.83ms
step:113/500 train_time:62633ms step_avg:554.27ms
step:114/500 train_time:63158ms step_avg:554.02ms
step:115/500 train_time:63722ms step_avg:554.10ms
step:116/500 train_time:64252ms step_avg:553.89ms
step:117/500 train_time:64819ms step_avg:554.01ms
step:118/500 train_time:65343ms step_avg:553.76ms
step:119/500 train_time:65815ms step_avg:553.06ms
step:120/500 train_time:66281ms step_avg:552.35ms
step:121/500 train_time:66890ms step_avg:552.81ms
step:122/500 train_time:67465ms step_avg:552.99ms
step:123/500 train_time:67931ms step_avg:552.29ms
step:124/500 train_time:68448ms step_avg:552.00ms
step:125/500 train_time:68953ms step_avg:551.62ms
step:126/500 train_time:69865ms step_avg:554.48ms
step:127/500 train_time:70387ms step_avg:554.23ms
step:128/500 train_time:71147ms step_avg:555.83ms
step:129/500 train_time:71605ms step_avg:555.08ms
step:130/500 train_time:72098ms step_avg:554.60ms
step:131/500 train_time:72866ms step_avg:556.23ms
step:132/500 train_time:73339ms step_avg:555.60ms
step:133/500 train_time:73842ms step_avg:555.20ms
step:134/500 train_time:74311ms step_avg:554.56ms
step:135/500 train_time:74893ms step_avg:554.77ms
step:136/500 train_time:75450ms step_avg:554.78ms
step:137/500 train_time:75926ms step_avg:554.20ms
step:138/500 train_time:76709ms step_avg:555.86ms
step:139/500 train_time:77241ms step_avg:555.69ms
step:140/500 train_time:77813ms step_avg:555.81ms
step:141/500 train_time:78292ms step_avg:555.26ms
step:142/500 train_time:78790ms step_avg:554.86ms
step:143/500 train_time:79538ms step_avg:556.21ms
step:144/500 train_time:80139ms step_avg:556.52ms
step:145/500 train_time:80603ms step_avg:555.88ms
step:146/500 train_time:81066ms step_avg:555.24ms
step:147/500 train_time:81538ms step_avg:554.68ms
step:148/500 train_time:82018ms step_avg:554.17ms
step:149/500 train_time:82488ms step_avg:553.61ms
step:150/500 train_time:82969ms step_avg:553.13ms
step:151/500 train_time:83495ms step_avg:552.95ms
step:152/500 train_time:84027ms step_avg:552.81ms
step:153/500 train_time:84555ms step_avg:552.65ms
step:154/500 train_time:85059ms step_avg:552.33ms
step:155/500 train_time:85536ms step_avg:551.84ms
step:156/500 train_time:86077ms step_avg:551.78ms
step:157/500 train_time:86723ms step_avg:552.37ms
step:158/500 train_time:87243ms step_avg:552.17ms
step:159/500 train_time:88021ms step_avg:553.59ms
step:160/500 train_time:88889ms step_avg:555.56ms
step:161/500 train_time:89464ms step_avg:555.68ms
step:162/500 train_time:90450ms step_avg:558.33ms
step:163/500 train_time:91181ms step_avg:559.39ms
step:164/500 train_time:91778ms step_avg:559.62ms
step:165/500 train_time:92305ms step_avg:559.42ms
step:166/500 train_time:92782ms step_avg:558.92ms
step:167/500 train_time:93249ms step_avg:558.38ms
step:168/500 train_time:93879ms step_avg:558.81ms
step:169/500 train_time:94667ms step_avg:560.16ms
step:170/500 train_time:95503ms step_avg:561.79ms
step:171/500 train_time:96161ms step_avg:562.35ms
step:172/500 train_time:96650ms step_avg:561.92ms
step:173/500 train_time:97109ms step_avg:561.32ms
step:174/500 train_time:97687ms step_avg:561.42ms
step:175/500 train_time:98383ms step_avg:562.19ms
step:176/500 train_time:98996ms step_avg:562.48ms
step:177/500 train_time:99458ms step_avg:561.91ms
step:178/500 train_time:99981ms step_avg:561.69ms
step:179/500 train_time:100447ms step_avg:561.16ms
step:180/500 train_time:100891ms step_avg:560.51ms
step:181/500 train_time:101506ms step_avg:560.81ms
step:182/500 train_time:101967ms step_avg:560.26ms
step:183/500 train_time:102426ms step_avg:559.71ms
step:184/500 train_time:102872ms step_avg:559.09ms
step:185/500 train_time:103388ms step_avg:558.86ms
step:186/500 train_time:103979ms step_avg:559.03ms
step:187/500 train_time:104530ms step_avg:558.98ms
step:188/500 train_time:104993ms step_avg:558.47ms
step:189/500 train_time:105468ms step_avg:558.03ms
step:190/500 train_time:105961ms step_avg:557.69ms
step:191/500 train_time:106411ms step_avg:557.12ms
step:192/500 train_time:107133ms step_avg:557.98ms
step:193/500 train_time:107718ms step_avg:558.12ms
step:194/500 train_time:108263ms step_avg:558.06ms
step:195/500 train_time:108767ms step_avg:557.78ms
step:196/500 train_time:109295ms step_avg:557.63ms
step:197/500 train_time:109778ms step_avg:557.25ms
step:198/500 train_time:110244ms step_avg:556.79ms
step:199/500 train_time:110962ms step_avg:557.60ms
step:200/500 train_time:111465ms step_avg:557.32ms
Validating on 524288 tokens (16 steps with 32768 batch size)
Total tokens across 1 shard(s): 100,000,000
Tokens needed for 500 iterations: 16,384,000
Training will use approximately 0.16 epochs over the data
step:200/500 val_loss:7.7194 train_time:113419ms step_avg:567.09ms
step:201/500 train_time:113433ms step_avg:564.34ms
step:202/500 train_time:113445ms step_avg:561.61ms
step:203/500 train_time:113458ms step_avg:558.90ms
step:204/500 train_time:113692ms step_avg:557.31ms
step:205/500 train_time:114392ms step_avg:558.01ms
step:206/500 train_time:114893ms step_avg:557.73ms
step:207/500 train_time:115612ms step_avg:558.51ms
step:208/500 train_time:116169ms step_avg:558.50ms
step:209/500 train_time:116624ms step_avg:558.01ms
step:210/500 train_time:117217ms step_avg:558.18ms
step:211/500 train_time:117735ms step_avg:557.99ms
step:212/500 train_time:118549ms step_avg:559.19ms
step:213/500 train_time:119236ms step_avg:559.79ms
step:214/500 train_time:119841ms step_avg:560.01ms
step:215/500 train_time:120363ms step_avg:559.83ms
step:216/500 train_time:120873ms step_avg:559.60ms
step:217/500 train_time:121338ms step_avg:559.16ms
step:218/500 train_time:121912ms step_avg:559.23ms
step:219/500 train_time:122432ms step_avg:559.05ms
step:220/500 train_time:123264ms step_avg:560.29ms
step:221/500 train_time:123865ms step_avg:560.47ms
step:222/500 train_time:124334ms step_avg:560.06ms
step:223/500 train_time:124947ms step_avg:560.30ms
step:224/500 train_time:125437ms step_avg:559.99ms
step:225/500 train_time:125898ms step_avg:559.55ms
step:226/500 train_time:126381ms step_avg:559.21ms
step:227/500 train_time:127158ms step_avg:560.17ms
step:228/500 train_time:127622ms step_avg:559.75ms
step:229/500 train_time:128353ms step_avg:560.50ms
step:230/500 train_time:129140ms step_avg:561.48ms
step:231/500 train_time:129978ms step_avg:562.67ms
step:232/500 train_time:130617ms step_avg:563.01ms
step:233/500 train_time:131156ms step_avg:562.90ms
step:234/500 train_time:131660ms step_avg:562.65ms
step:235/500 train_time:132111ms step_avg:562.17ms
step:236/500 train_time:132599ms step_avg:561.86ms
step:237/500 train_time:133178ms step_avg:561.93ms
step:238/500 train_time:133743ms step_avg:561.95ms
step:239/500 train_time:134215ms step_avg:561.57ms
step:240/500 train_time:134981ms step_avg:562.42ms
step:241/500 train_time:135449ms step_avg:562.03ms
step:242/500 train_time:135953ms step_avg:561.79ms
step:243/500 train_time:136414ms step_avg:561.37ms
step:244/500 train_time:136893ms step_avg:561.04ms
step:245/500 train_time:137370ms step_avg:560.70ms
step:246/500 train_time:137838ms step_avg:560.32ms
step:247/500 train_time:138339ms step_avg:560.08ms
step:248/500 train_time:139013ms step_avg:560.54ms
step:249/500 train_time:139803ms step_avg:561.46ms
step:250/500 train_time:140618ms step_avg:562.47ms
step:251/500 train_time:141278ms step_avg:562.86ms
step:252/500 train_time:141830ms step_avg:562.82ms
step:253/500 train_time:142315ms step_avg:562.51ms
step:254/500 train_time:143138ms step_avg:563.54ms
step:255/500 train_time:143590ms step_avg:563.10ms
step:256/500 train_time:144052ms step_avg:562.70ms
step:257/500 train_time:144568ms step_avg:562.52ms
step:258/500 train_time:145038ms step_avg:562.16ms
step:259/500 train_time:145499ms step_avg:561.77ms
step:260/500 train_time:146039ms step_avg:561.69ms
step:261/500 train_time:146524ms step_avg:561.39ms
step:262/500 train_time:146974ms step_avg:560.97ms
step:263/500 train_time:147558ms step_avg:561.06ms
step:264/500 train_time:148042ms step_avg:560.76ms
step:265/500 train_time:148509ms step_avg:560.41ms
step:266/500 train_time:149108ms step_avg:560.56ms
step:267/500 train_time:149572ms step_avg:560.20ms
step:268/500 train_time:150079ms step_avg:560.00ms
step:269/500 train_time:150609ms step_avg:559.89ms
step:270/500 train_time:151115ms step_avg:559.68ms
step:271/500 train_time:151579ms step_avg:559.33ms
step:272/500 train_time:152022ms step_avg:558.91ms
step:273/500 train_time:152529ms step_avg:558.71ms
step:274/500 train_time:153017ms step_avg:558.46ms
step:275/500 train_time:153496ms step_avg:558.17ms
step:276/500 train_time:154136ms step_avg:558.46ms
step:277/500 train_time:154633ms step_avg:558.24ms
step:278/500 train_time:155115ms step_avg:557.97ms
step:279/500 train_time:155662ms step_avg:557.93ms
step:280/500 train_time:156185ms step_avg:557.80ms
step:281/500 train_time:156665ms step_avg:557.53ms
step:282/500 train_time:157297ms step_avg:557.79ms
step:283/500 train_time:157795ms step_avg:557.58ms
step:284/500 train_time:158281ms step_avg:557.33ms
step:285/500 train_time:158752ms step_avg:557.03ms
step:286/500 train_time:159225ms step_avg:556.73ms
step:287/500 train_time:159689ms step_avg:556.41ms
step:288/500 train_time:160174ms step_avg:556.16ms
step:289/500 train_time:160664ms step_avg:555.93ms
step:290/500 train_time:161275ms step_avg:556.12ms
step:291/500 train_time:161735ms step_avg:555.79ms
step:292/500 train_time:162395ms step_avg:556.15ms
step:293/500 train_time:162942ms step_avg:556.11ms
step:294/500 train_time:163443ms step_avg:555.93ms
step:295/500 train_time:163926ms step_avg:555.68ms
step:296/500 train_time:164498ms step_avg:555.73ms
step:297/500 train_time:164969ms step_avg:555.45ms
step:298/500 train_time:165663ms step_avg:555.92ms
step:299/500 train_time:166247ms step_avg:556.01ms
step:300/500 train_time:166890ms step_avg:556.30ms
Validating on 524288 tokens (16 steps with 32768 batch size)
Total tokens across 1 shard(s): 100,000,000
Tokens needed for 500 iterations: 16,384,000
Training will use approximately 0.16 epochs over the data
step:300/500 val_loss:7.3636 train_time:168801ms step_avg:562.67ms
step:301/500 train_time:168816ms step_avg:560.85ms
step:302/500 train_time:168828ms step_avg:559.03ms
step:303/500 train_time:168840ms step_avg:557.23ms
step:304/500 train_time:169082ms step_avg:556.19ms
step:305/500 train_time:169557ms step_avg:555.92ms
step:306/500 train_time:170002ms step_avg:555.56ms
step:307/500 train_time:170457ms step_avg:555.23ms
step:308/500 train_time:170944ms step_avg:555.01ms
step:309/500 train_time:171457ms step_avg:554.88ms
step:310/500 train_time:171915ms step_avg:554.56ms
step:311/500 train_time:172711ms step_avg:555.34ms
step:312/500 train_time:173194ms step_avg:555.11ms
step:313/500 train_time:173651ms step_avg:554.80ms
step:314/500 train_time:174116ms step_avg:554.51ms
step:315/500 train_time:174633ms step_avg:554.39ms
step:316/500 train_time:175094ms step_avg:554.09ms
step:317/500 train_time:175670ms step_avg:554.16ms
step:318/500 train_time:176148ms step_avg:553.92ms
step:319/500 train_time:176711ms step_avg:553.95ms
step:320/500 train_time:177169ms step_avg:553.65ms
step:321/500 train_time:177665ms step_avg:553.47ms
step:322/500 train_time:178125ms step_avg:553.18ms
step:323/500 train_time:178762ms step_avg:553.44ms
step:324/500 train_time:179282ms step_avg:553.34ms
step:325/500 train_time:179861ms step_avg:553.42ms
step:326/500 train_time:180652ms step_avg:554.15ms
step:327/500 train_time:181202ms step_avg:554.13ms
step:328/500 train_time:181722ms step_avg:554.03ms
step:329/500 train_time:182576ms step_avg:554.94ms
step:330/500 train_time:183090ms step_avg:554.82ms
step:331/500 train_time:183691ms step_avg:554.96ms
step:332/500 train_time:184235ms step_avg:554.93ms
step:333/500 train_time:184722ms step_avg:554.72ms
step:334/500 train_time:185189ms step_avg:554.46ms
step:335/500 train_time:185908ms step_avg:554.95ms
step:336/500 train_time:186525ms step_avg:555.13ms
step:337/500 train_time:187052ms step_avg:555.05ms
step:338/500 train_time:187652ms step_avg:555.18ms
step:339/500 train_time:188201ms step_avg:555.17ms
step:340/500 train_time:188930ms step_avg:555.68ms
step:341/500 train_time:189648ms step_avg:556.15ms
step:342/500 train_time:190170ms step_avg:556.05ms
step:343/500 train_time:190850ms step_avg:556.41ms
step:344/500 train_time:191309ms step_avg:556.13ms
step:345/500 train_time:191753ms step_avg:555.81ms
step:346/500 train_time:192221ms step_avg:555.55ms
step:347/500 train_time:192735ms step_avg:555.43ms
step:348/500 train_time:193224ms step_avg:555.24ms
step:349/500 train_time:193700ms step_avg:555.01ms
step:350/500 train_time:194529ms step_avg:555.80ms
step:351/500 train_time:194985ms step_avg:555.51ms
step:352/500 train_time:195472ms step_avg:555.32ms
step:353/500 train_time:196010ms step_avg:555.27ms
step:354/500 train_time:196495ms step_avg:555.07ms
step:355/500 train_time:196955ms step_avg:554.80ms
step:356/500 train_time:197576ms step_avg:554.99ms
step:357/500 train_time:198308ms step_avg:555.48ms
step:358/500 train_time:198768ms step_avg:555.22ms
step:359/500 train_time:199415ms step_avg:555.47ms
step:360/500 train_time:200069ms step_avg:555.75ms
step:361/500 train_time:200695ms step_avg:555.94ms
step:362/500 train_time:201161ms step_avg:555.69ms
step:363/500 train_time:201685ms step_avg:555.61ms
step:364/500 train_time:202175ms step_avg:555.43ms
step:365/500 train_time:202650ms step_avg:555.20ms
step:366/500 train_time:203290ms step_avg:555.44ms
step:367/500 train_time:203833ms step_avg:555.40ms
step:368/500 train_time:204333ms step_avg:555.25ms
step:369/500 train_time:204982ms step_avg:555.51ms
step:370/500 train_time:205462ms step_avg:555.30ms
step:371/500 train_time:205956ms step_avg:555.14ms
step:372/500 train_time:206640ms step_avg:555.48ms
step:373/500 train_time:207207ms step_avg:555.51ms
step:374/500 train_time:207684ms step_avg:555.31ms
step:375/500 train_time:208157ms step_avg:555.09ms
step:376/500 train_time:208650ms step_avg:554.92ms
step:377/500 train_time:209130ms step_avg:554.72ms
step:378/500 train_time:209668ms step_avg:554.68ms
step:379/500 train_time:210161ms step_avg:554.51ms
step:380/500 train_time:210669ms step_avg:554.39ms
step:381/500 train_time:211138ms step_avg:554.17ms
step:382/500 train_time:212038ms step_avg:555.07ms
step:383/500 train_time:212721ms step_avg:555.41ms
step:384/500 train_time:213342ms step_avg:555.58ms
step:385/500 train_time:214123ms step_avg:556.16ms
step:386/500 train_time:214624ms step_avg:556.02ms
step:387/500 train_time:215156ms step_avg:555.96ms
step:388/500 train_time:215694ms step_avg:555.91ms
step:389/500 train_time:216155ms step_avg:555.67ms
step:390/500 train_time:216664ms step_avg:555.55ms
step:391/500 train_time:217129ms step_avg:555.32ms
step:392/500 train_time:217592ms step_avg:555.08ms
step:393/500 train_time:218077ms step_avg:554.90ms
step:394/500 train_time:218740ms step_avg:555.18ms
step:395/500 train_time:219225ms step_avg:555.00ms
step:396/500 train_time:219668ms step_avg:554.72ms
step:397/500 train_time:220232ms step_avg:554.74ms
step:398/500 train_time:220798ms step_avg:554.77ms
step:399/500 train_time:221338ms step_avg:554.73ms
step:400/500 train_time:221844ms step_avg:554.61ms
Validating on 524288 tokens (16 steps with 32768 batch size)
Total tokens across 1 shard(s): 100,000,000
Tokens needed for 500 iterations: 16,384,000
Training will use approximately 0.16 epochs over the data
step:400/500 val_loss:7.2218 train_time:223980ms step_avg:559.95ms
step:401/500 train_time:223994ms step_avg:558.59ms
step:402/500 train_time:224006ms step_avg:557.23ms
step:403/500 train_time:224018ms step_avg:555.88ms
step:404/500 train_time:224472ms step_avg:555.62ms
step:405/500 train_time:225312ms step_avg:556.33ms
step:406/500 train_time:226151ms step_avg:557.02ms
step:407/500 train_time:226763ms step_avg:557.16ms
step:408/500 train_time:227306ms step_avg:557.12ms
step:409/500 train_time:227787ms step_avg:556.94ms
step:410/500 train_time:228464ms step_avg:557.23ms
step:411/500 train_time:229148ms step_avg:557.54ms
step:412/500 train_time:229717ms step_avg:557.57ms
step:413/500 train_time:230248ms step_avg:557.50ms
step:414/500 train_time:230707ms step_avg:557.26ms
step:415/500 train_time:231328ms step_avg:557.42ms
step:416/500 train_time:231840ms step_avg:557.31ms
step:417/500 train_time:232330ms step_avg:557.15ms
step:418/500 train_time:232837ms step_avg:557.03ms
step:419/500 train_time:233304ms step_avg:556.81ms
step:420/500 train_time:234131ms step_avg:557.45ms
step:421/500 train_time:234785ms step_avg:557.68ms
step:422/500 train_time:235350ms step_avg:557.70ms
step:423/500 train_time:235813ms step_avg:557.48ms
step:424/500 train_time:236311ms step_avg:557.34ms
step:425/500 train_time:236800ms step_avg:557.18ms
step:426/500 train_time:237371ms step_avg:557.21ms
step:427/500 train_time:238159ms step_avg:557.75ms
step:428/500 train_time:238737ms step_avg:557.80ms
step:429/500 train_time:239202ms step_avg:557.58ms
step:430/500 train_time:239819ms step_avg:557.72ms
step:431/500 train_time:240631ms step_avg:558.31ms
step:432/500 train_time:241288ms step_avg:558.54ms
step:433/500 train_time:241768ms step_avg:558.36ms
step:434/500 train_time:242328ms step_avg:558.36ms
step:435/500 train_time:243124ms step_avg:558.91ms
step:436/500 train_time:243834ms step_avg:559.25ms
step:437/500 train_time:244583ms step_avg:559.69ms
step:438/500 train_time:245175ms step_avg:559.76ms
step:439/500 train_time:246008ms step_avg:560.38ms
step:440/500 train_time:246553ms step_avg:560.35ms
step:441/500 train_time:247036ms step_avg:560.17ms
step:442/500 train_time:247537ms step_avg:560.04ms
step:443/500 train_time:248187ms step_avg:560.24ms
step:444/500 train_time:248686ms step_avg:560.10ms
step:445/500 train_time:249137ms step_avg:559.86ms
step:446/500 train_time:249596ms step_avg:559.63ms
step:447/500 train_time:250184ms step_avg:559.70ms
step:448/500 train_time:250905ms step_avg:560.06ms
step:449/500 train_time:251495ms step_avg:560.12ms
step:450/500 train_time:251973ms step_avg:559.94ms
step:451/500 train_time:252476ms step_avg:559.81ms
step:452/500 train_time:252943ms step_avg:559.61ms
step:453/500 train_time:253403ms step_avg:559.39ms
step:454/500 train_time:253893ms step_avg:559.24ms
step:455/500 train_time:254392ms step_avg:559.10ms
step:456/500 train_time:254864ms step_avg:558.91ms
step:457/500 train_time:255319ms step_avg:558.68ms
step:458/500 train_time:255779ms step_avg:558.47ms
step:459/500 train_time:256251ms step_avg:558.28ms
step:460/500 train_time:256714ms step_avg:558.07ms
step:461/500 train_time:257428ms step_avg:558.41ms
step:462/500 train_time:257939ms step_avg:558.31ms
step:463/500 train_time:258391ms step_avg:558.08ms
step:464/500 train_time:258846ms step_avg:557.86ms
step:465/500 train_time:259323ms step_avg:557.68ms
step:466/500 train_time:259972ms step_avg:557.88ms
step:467/500 train_time:260452ms step_avg:557.71ms
step:468/500 train_time:261024ms step_avg:557.74ms
step:469/500 train_time:261563ms step_avg:557.70ms
step:470/500 train_time:262229ms step_avg:557.93ms
step:471/500 train_time:262820ms step_avg:558.00ms
step:472/500 train_time:263556ms step_avg:558.38ms
step:473/500 train_time:264395ms step_avg:558.98ms
step:474/500 train_time:265192ms step_avg:559.48ms
step:475/500 train_time:265844ms step_avg:559.67ms
step:476/500 train_time:266338ms step_avg:559.53ms
step:477/500 train_time:266818ms step_avg:559.37ms
step:478/500 train_time:267284ms step_avg:559.17ms
step:479/500 train_time:267771ms step_avg:559.02ms
step:480/500 train_time:268503ms step_avg:559.38ms
step:481/500 train_time:268961ms step_avg:559.17ms
step:482/500 train_time:269408ms step_avg:558.94ms
step:483/500 train_time:270138ms step_avg:559.29ms
step:484/500 train_time:270978ms step_avg:559.87ms
step:485/500 train_time:271818ms step_avg:560.45ms
step:486/500 train_time:272657ms step_avg:561.02ms
step:487/500 train_time:273497ms step_avg:561.60ms
step:488/500 train_time:274336ms step_avg:562.16ms
step:489/500 train_time:275175ms step_avg:562.73ms
step:490/500 train_time:275978ms step_avg:563.22ms
step:491/500 train_time:276561ms step_avg:563.26ms
step:492/500 train_time:277179ms step_avg:563.37ms
step:493/500 train_time:277817ms step_avg:563.52ms
step:494/500 train_time:278358ms step_avg:563.48ms
step:495/500 train_time:278860ms step_avg:563.35ms
step:496/500 train_time:279541ms step_avg:563.59ms
step:497/500 train_time:280168ms step_avg:563.72ms
step:498/500 train_time:280684ms step_avg:563.62ms
step:499/500 train_time:281158ms step_avg:563.44ms
step:500/500 train_time:281787ms step_avg:563.57ms
Validating on 524288 tokens (16 steps with 32768 batch size)
Total tokens across 1 shard(s): 100,000,000
Tokens needed for 500 iterations: 16,384,000
Training will use approximately 0.16 epochs over the data
step:500/500 val_loss:7.1618 train_time:284379ms step_avg:568.76ms
peak memory allocated: 8879 MiB reserved: 9118 MiB
---------- making sure assertions worked ----------
tensor([0.1220, 0.0657, 0.1733, 0.0925, 0.0808], device='cuda:0')
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0',
       grad_fn=<SliceBackward0>)
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0',
       grad_fn=<SliceBackward0>)
Once upon a time, the its school, a these in as, it and be a disease a results
The meaning of life is in the on the its the common because to order, with should be as the
In the year 2026,. the, a used in the the, the the other time the key�
I'm a Large Language Model (LLM), which means and will its some, time of the (The little important. A,.
Found HellaSwag dataset at ./data/hellaswag/hellaswag_val.jsonl, running evaluation...
Starting HellaSwag evaluation...
---
Context:
 A man is sitting on a roof. he
Endings:
0 (loss: 6.9352) is using wrap to wrap a pair of skis.
1 (loss: 8.3850) is ripping level tiles off.
2 (loss: 7.1909) is holding a rubik's cube.
3 (loss: 7.4076) starts pulling up roofing on a roof.
predicted: 0, actual: 3
---
Context:
 A lady walks to a barbell. She bends down and grabs the pole. the lady
Endings:
0 (loss: 6.9638) swings and lands in her arms.
1 (loss: 7.9325) pulls the barbell forward.
2 (loss: 7.4521) pulls a rope attached to the barbell.
3 (loss: 6.9009) stands and lifts the weight over her head.
predicted: 3, actual: 3
---
Context:
 Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. the child and a different man
Endings:
0 (loss: 6.9675) are then shown paddling down a river in a boat while a woman talks.
1 (loss: 6.5187) are driving the canoe, they go down the river flowing side to side.
2 (loss: 7.0575) sit in a canoe while the man paddles.
3 (loss: 7.1643) walking go down the rapids, while the man in his helicopter almost falls and goes out of canoehood.
predicted: 1, actual: 2
---
Context:
 A boy is running down a track. the boy
Endings:
0 (loss: 6.4268) runs into a car.
1 (loss: 6.5094) gets in a mat.
2 (loss: 6.5803) lifts his body above the height of a pole.
3 (loss: 6.7172) stands on his hands and springs.
predicted: 0, actual: 2
---
Context:
 The boy lifts his body above the height of a pole. The boy lands on his back on to a red mat. the boy
Endings:
0 (loss: 6.8145) turns his body around on the mat.
1 (loss: 6.5442) gets up from the mat.
2 (loss: 6.6282) continues to lift his body over the pole.
3 (loss: 6.9689) wiggles out of the mat.
predicted: 1, actual: 1
