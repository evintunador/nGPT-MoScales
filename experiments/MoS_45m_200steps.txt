import os
import sys
with open(sys.argv[0]) as f:
    code = f.read() # read the code of this file ASAP, for logging
import uuid
import time
import copy
import glob
from dataclasses import dataclass
from functools import lru_cache
from pathlib import Path
import itertools
import tiktoken
import json
import math

os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
import torch
torch.empty(1, device="cuda", requires_grad=True).backward() # prevents a bug on some systems
torch.set_float32_matmul_precision('high')
from torch import Tensor, nn
import torch.nn.functional as F
import torch.distributed as dist
from torch.nn.attention.flex_attention import BlockMask, flex_attention, create_block_mask

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

from kernels.cos_norm import cosine_norm_triton_ as cosine_norm_inplace_
from kernels.cos_norm import cosine_norm_triton as cosine_norm
#from kernels.resid import resid_fwd_naive as resid
from kernels.resid import resid_fwd_triton as resid
#from kernels.resid import resid_fwd_frankenstein as resid # out of VRAM error bc of bwd

class Scale(nn.Module):
    """
    A module that manages learnable scaling parameters to ensure different learning rates
    from the rest of the parameters in the model (see pages 5 and 19)
    
    Args:
        dim (int): Dimension of the scaling parameter
        scale (float): Initial scale value
        init (float): Initial value for the scaling parameter
        device (str, optional): Device to store the parameter on
    """
    def __init__(self, dim: int, heads: int = 1, scale: float = 1.0, init: float = 1.0):
        super().__init__()
        self.init = init
        self.scale = scale
        self.s = nn.Parameter((torch.ones(heads, dim) if heads > 1 else torch.ones(dim)) * scale)
            # heads == 1 gives us a single regular vector
            # heads > 1 gets used in attention mechanism for different scaling vector for each head
    
    def forward(self):
        """Compute the effective scaling factor."""
        return self.s * (self.init / self.scale) # shape (heads, dim)

class MoScale(nn.Module):
    def __init__(self, dim, rank):
        super().__init__()
        assert rank < dim
        self.w = nn.Linear(dim, rank, bias=True)

    def forward(self, x): # (b, n, d)
        probs = F.softmax(self.w(x), dim=-1) # (b, n, r) where r << d
        scales = probs.unsqueeze(2) * self.w.weight.T.unsqueeze(0) # (b, n, 1, r) * (1, d, r) -> (b, n, d, r)
        scale = torch.sum(scales, dim=-1).squeeze(-1) # (b, n, d)
        return scale

class Rotary(nn.Module):
    def __init__(self, dim: int, max_seq_len: int):
        super().__init__()
        # Ensure we don't exceed the dimension size
        dim_quarter = max(1, dim // 4)
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=dim_quarter, dtype=torch.float32)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(dim_quarter)])
        t = torch.arange(max_seq_len, dtype=torch.float32)
        theta = torch.einsum("i,j -> ij", t, angular_freq) # outer product
        self.cos = nn.Buffer(theta.cos(), persistent=False)
        self.sin = nn.Buffer(theta.sin(), persistent=False)

    def forward(self, x_BTHD: Tensor):
        assert self.cos.size(0) >= x_BTHD.size(-3)
        cos, sin = self.cos[None, :x_BTHD.size(-3), None, :], self.sin[None, :x_BTHD.size(-3), None, :]
        # Handle case where the number of dimensions is smaller
        dim_half = x_BTHD.size(-1) // 2
        x1, x2 = x_BTHD.to(dtype=torch.float32).chunk(2, dim=-1)
        y1 = x1 * cos[..., :dim_half] + x2 * sin[..., :dim_half]
        y2 = x1 * (-sin[..., :dim_half]) + x2 * cos[..., :dim_half]
        return torch.cat((y1, y2), 3).type_as(x_BTHD)

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, num_heads: int, max_seq_len: int, head_dim=None):
        super().__init__()
        # Calculate head_dim based on model dimensions and num_heads
        self.num_heads = num_heads
        # If head_dim not specified, calculate it based on the model dimension
        self.head_dim = dim // num_heads if head_dim is None else head_dim
        self.Wq = nn.Linear(dim, num_heads * self.head_dim, bias=False)
        self.Wk = nn.Linear(dim, num_heads * self.head_dim, bias=False)
        self.Wv = nn.Linear(dim, num_heads * self.head_dim, bias=False)
        # the scaling factor to apply to the normalized queries & keys (see page 4)
        self.s_qk = Scale(self.head_dim, heads=num_heads, scale = 1. / math.sqrt(dim))
        # the scaling factor to apply to the attention logits to restore a variance of 1 (see page 4)
        self.scale = self.head_dim ** 0.5
        self.rotary = Rotary(self.head_dim, max_seq_len)
        self.Wo = nn.Linear(num_heads * self.head_dim, dim, bias=False)

    def forward(self, x: Tensor, block_mask: BlockMask):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "Must use batch size = 1 for FlexAttention"
        # Linear projections for queries, keys, and values
        q, k, v = self.Wq(x), self.Wk(x), self.Wv(x)
            # shape: (batch_size, seq_len, dim) -> (batch_size, seq_len, num_heads * head_dim)
        # Reshape projections to separate heads
        q = q.view(B, T, self.num_heads, self.head_dim)
        k = k.view(B, T, self.num_heads, self.head_dim)
        v = v.view(B, T, self.num_heads, self.head_dim)
        # normalizing & scaling our queries  & keys (see page 4)
        s_qk = self.s_qk() # (num_heads, head_dim)
        q = cosine_norm(q) * s_qk # then scale each head
        k = cosine_norm(k) * s_qk # no shape change
        # apply RoPE
        q, k = self.rotary(q), self.rotary(k)
        # the meat of the attention calculation
        y = flex_attention(q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), block_mask=block_mask, scale=0.12).transpose(1, 2)
        # combine heads
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        # mix heads
        y = self.Wo(y)
        return y

class MLP(nn.Module):
    def __init__(self, dim: int, mlp_ratio: int = 4):
        super().__init__()
        hdim = int(mlp_ratio * dim)
        self.Wup = nn.Linear(dim, hdim, bias=False)
        self.Wgate = nn.Linear(dim, hdim, bias=False)
        self.Wdown = nn.Linear(hdim, dim, bias=False)
        # this flag designates Wdown to have a different parameter initialization as defined in model.py
        self.Wdown.GPT_scale_init = 1 
        # the learnable scaling factors
        self.s_u = Scale(hdim)
        self.s_v = Scale(hdim)
        # the varaince-controlling scaling term, needed to benefit from SiLU (see appendix A.1)
        self.scale = math.sqrt(dim)

    def forward(self, x: Tensor):
        # our up & gate projections
        u = self.Wup(x) # (batch_size, seq_len, hidden_dim)
        v = self.Wgate(x)
        # scale them
        u = u * self.s_u()
        v = v * self.s_v() * self.scale 
        # now perform the nonlinearity gate
        hidden = u * F.silu(v) # (batch_size, seq_len, hidden_dim)
        return self.Wdown(hidden) # (batch_size, seq_len, output_dim)

class Block(nn.Module):
    def __init__(self, dim: int, num_heads: int, mlp_ratio: int, max_seq_len: int, eigen_rank: int, layer_idx: int):
        super().__init__()
        self.attn = CausalSelfAttention(dim, num_heads, max_seq_len)
        self.mlp = MLP(dim, mlp_ratio)

        #self.alpha_A = Scale(dim, init = 0.05, scale = 1. / math.sqrt(dim))
            # not sure what scale to use with a_A and a_M. At one point i had it as 1./math.sqrt(cfg.dim)
            # but now i can't find the reference to that in the paper
        # eigen learning rate vector
        #self.alpha_M = Scale(dim, init = 0.05, scale = 1. / math.sqrt(dim))
        self.alpha_A = MoScale(dim, eigen_rank)
        self.alpha_M = MoScale(dim, eigen_rank)

    def forward(self, x: Tensor, block_mask: BlockMask):
        x_A = cosine_norm(self.attn(x, block_mask))
        x = cosine_norm(x + self.alpha_A(x) * (x_A - x))
        #x = resid(x, self.attn(x, block_mask), self.alpha_A(x))
        x_M = cosine_norm(self.mlp(x))
        x = cosine_norm(x + self.alpha_M(x) * (x_M - x))
        #x = resid(x, self.mlp(x), self.alpha_M(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, model_dim: int, max_seq_len: int, mlp_ratio: int, eigen_rank: int):
        super().__init__()
        self.max_seq_len = max_seq_len
        self.model_dim = model_dim
        self.vocab_size = vocab_size
        self.embed = nn.Embedding(vocab_size, model_dim)
        self.blocks = nn.ModuleList([Block(model_dim, num_heads, mlp_ratio, max_seq_len, eigen_rank, i) for i in range(num_layers)])
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency. this originates from Karpathy's experiments.
        self.lm_head = nn.Linear(model_dim, next_multiple_of_n(vocab_size, n=128))
        # scaling param to un-limit the range for the final probability distribution (see page 2)
        self.s_z = Scale(next_multiple_of_n(vocab_size, n=128), scale = 1./math.sqrt(model_dim))
        # initializing params to specific distributions
        self.apply(self.__init__weights)

    def __init__weights(self, module):
        """
        parameter initialization isn't actually important in N-GPT because of the normalization
        However we'll still initialize according to how they did in appendix A.5
        """
        # whereas GPT-2 used std = 0.02, we'll do square root of model's embedding dimension
        std = math.sqrt(self.model_dim) 

        if isinstance(module, (nn.Linear, nn.Parameter)):
            # specific weight matrices at the end of each layer are given smaller std 
            # originally this was done in GPT-2 to keep the residual stream small
            if hasattr(module, 'GPT_scale_init'):
                std *= (2 * len(self.blocks)) ** -0.5

            # carries out the actual initialization
            torch.nn.init.normal_(module.weight, mean=0.0, std=std)

            # biases, if any, should instead be initialized to zeros
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias) 

        # the embedding matrix doesn't count as an nn.Linear so we've gotta do it again for that
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=std)

    def forward(self, input_seq: Tensor, target_seq: Tensor = None):
        assert input_seq.ndim == 1

        docs = (input_seq == 50256).cumsum(0)
        def document_causal(b, h, q_idx, kv_idx):
            causal_mask = q_idx >= kv_idx
            document_mask = docs[q_idx] == docs[kv_idx]
            return causal_mask & document_mask
        doc_causal_mask = create_block_mask(document_causal, B=None, H=None, Q_LEN=input_seq.size(0), KV_LEN=input_seq.size(0))
      
        x = self.embed(input_seq)[None]

        for i in range(len(self.blocks)):
            x = self.blocks[i](x, doc_causal_mask)

        logits = self.lm_head(x).float()
        # to un-limit the temperature of the final probability distribution (see page 2)
        scaled_logits = logits * self.s_z()
        
        if target_seq is None:
            return scaled_logits
        else:
            return F.cross_entropy(scaled_logits.view(-1, logits.size(-1)), target_seq, 
                                  reduction='sum' if self.training else 'mean')

    def get_num_params(self):
        """
        Return the number of parameters in the model.
        """
        n_params = sum(p.numel() for p in self.parameters())
        return n_params
    
    def normalize_linear(self, module):
        """
        Helper method to normalize Linear layer weights where one dimension matches model dim
        """
        # Find the dimension that matches cfg.dim
        dim_to_normalize = None
        for dim, size in enumerate(module.weight.shape):
            if size == self.model_dim:
                dim_to_normalize = dim
                break
        
        if dim_to_normalize is not None:
            # Normalize the weights in-place
            cosine_norm_inplace_(module.weight.data, dim=dim_to_normalize)

    def enforce_constraints(self):
        """
        Enforces constraints after each optimization step:
        1. Absolute value constraint on eigen learning rate parameters
        2. Cosine normalization on Linear layer weights where one dimension matches model dim
        """
        with torch.no_grad():
            # Enforce absolute value on eigen learning rates
            #for layer in self.blocks:
                #layer.alpha_A.s.data.abs_()
                #layer.alpha_M.s.data.abs_()
            
            # Cosine normalize relevant Linear layers
            for module in self.modules():
                if isinstance(module, (nn.Linear, nn.Embedding)):
                    self.normalize_linear(module)

    @torch.no_grad()
    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):
        """
        Take a conditioning sequence of indices idx (LongTensor of shape (t)) and complete
        the sequence max_new_tokens times, feeding the predictions back into the model each time.
        Most likely you'll want to make sure to be in model.eval() mode of operation for this.
        """
        assert idx.ndim == 1
        def cdiv(m, n):
            return (m + (n - 1)) // n
        seq_len = idx.size(0)
        if seq_len % 128 != 0:
            pad_ct = cdiv(seq_len, 128) * 128 - seq_len
            idx = torch.cat((idx, torch.zeros(pad_ct, dtype=idx.dtype, device=idx.device)), dim=0)
        
        self.eval()  # Ensure model is in evaluation mode
        for _ in range(max_new_tokens):
            # Forward pass to get logits
            logits = self(idx[-self.max_seq_len:] if idx.size(0) > self.max_seq_len else idx)
            # Focus on the last token's prediction
            logits = logits[0, min(seq_len, self.max_seq_len) - 1, :] / temperature
            # optionally crop the logits to only the top k options
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[-1]] = -float('Inf')
            # apply softmax to convert logits to (normalized) probabilities
            probs = F.softmax(logits, dim=-1)
            # sample from the distribution
            idx_next = torch.multinomial(probs, num_samples=1)
            while idx_next >= self.vocab_size: # don't want to grab any of the out-of-vocab tokens
                idx_next = torch.multinomial(probs, num_samples=1)
                # could get stuck in an infinite loop here bit that's hella unlikely & i'm lazy
            # append sampled index to the running sequence and continue
            idx[min(seq_len, self.max_seq_len)] = idx_next

            # iterate sequence count and account for any time we surpass flex-attention's block size
            seq_len += 1
            if (seq_len - 1) % 128 == 0:
                pad_ct = cdiv(seq_len, 128) * 128 - seq_len
                idx = torch.cat((idx, [0] * pad_ct), dim=0)

        return idx[:seq_len]

# -----------------------------------------------------------------------------
# Our own simple Distributed Data Loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

def distributed_data_generator(filename_pattern: str, batch_size: int, rank: int, world_size: int):
    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise ValueError(f"No files found matching pattern: {filename_pattern}")
    
    assert batch_size % world_size == 0
    local_batch_size = batch_size // world_size
    
    # Calculate total tokens across all shards
    total_tokens = 0
    tokens_per_file = []
    for file in files:
        header = torch.from_file(str(file), False, 256, dtype=torch.int32)
        file_tokens = int(header[2])
        total_tokens += file_tokens
        tokens_per_file.append(file_tokens)
    
    # Calculate how many tokens we need for training
    tokens_needed = args.num_iterations * batch_size
    
    # Determine if we need to cycle and calculate epochs
    will_cycle = total_tokens < tokens_needed
    epochs = tokens_needed / total_tokens if total_tokens > 0 else 0
    
    if rank == 0:
        print0(f"Total tokens across {len(files)} shard(s): {total_tokens:,}", console=True)
        print0(f"Tokens needed for {args.num_iterations} iterations: {tokens_needed:,}", console=True)
        print0(f"Training will use approximately {epochs:.2f} epochs over the data", console=True)
    
    file_iter = itertools.cycle(files) if will_cycle else iter(files)
    tokens, pos = _load_data_shard(next(file_iter)), 0
    
    while True:
        if pos + batch_size + 1 >= len(tokens):
            tokens, pos = _load_data_shard(next(file_iter)), 0
        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]
        inputs = buf[:-1].to(device="cuda", dtype=torch.int32, non_blocking=True) # no sync on host side;
        targets = buf[1:].to(device="cuda", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.
        pos += batch_size
        yield inputs, targets

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files = "data/fineweb*10B/fineweb*_train_*.bin" # input .bin to train on
    val_files = "data/fineweb*10B/fineweb*_val_*.bin" # input .bin to eval validation loss on
    val_tokens = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    train_seq_len = 8*1024 # FlexAttention sequence length - reduced from 48*1024 for GPUs w/ at least 8GB VRAM during testing
    val_seq_len = 8*1024 # FlexAttention sequence length for validation - reduced from 4*64*1024
    # optimization
    num_iterations = 200 # number of iterations to run
    lr_init = 0.001
    lr_final = 0.0001
    # architecture
    vocab_size = 50257
    # model size - setup for GPUs w/ 8GB of VRAM
    num_layers = 4
    num_heads = 6
    model_dim = 384
    head_dim = None  # if None, will be set to model_dim // num_heads
    mlp_ratio = int(4 * 2/3) # 2/3 to make the GLU number of parameters eqiuvalent to not GLU
    # evaluation and logging
    val_loss_every = 100 # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint = False
    eigen_rank = 32

    def __post_init__(self):
        # Validate and set derived parameters
        if self.head_dim is None:
            self.head_dim = self.model_dim // self.num_heads
        assert self.head_dim in [2 ** i for i in range(1, 10)], f"head_dim must be a power of 2, got {self.head_dim}"
        assert self.mlp_ratio > 0, f"mlp_ratio must be positive, got {self.mlp_ratio}"
        assert self.train_seq_len % 128 == 0, f"train_seq_len must be multiple of 128, got {self.train_seq_len}"
        assert self.val_seq_len % 128 == 0, f"val_seq_len must be multiple of 128, got {self.val_seq_len}"

args = Hyperparameters()

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
# Remove assertion for 8xH100
# assert world_size == 8 # this code is designed for 8xH100
print(f"Running with {world_size} GPUs (designed originally for 8xH100, adapted to also support 2x GPUs w/ at least 8GB VRAM during testing)")
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = uuid.uuid4()
    os.makedirs("experiments", exist_ok=True)  # Changed from "logs" to "experiments"
    logfile = f"experiments/{run_id}.txt"  # Changed from "logs/" to "experiments/"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing all relevant files
print0(code)  # Print this file's code

# Print hellaswag.py if it exists
try:
    with open("hellaswag.py", "r") as f:
        print0("\n" + "="*100 + "\nhellaswag.py:\n" + "="*100)
        print0(f.read())
except FileNotFoundError:
    print0("\n" + "="*100 + "\nhellaswag.py not found\n" + "="*100)

print0("="*100)

# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

########################################
#    Construct model and optimizer     #
########################################

model: nn.Module = GPT(vocab_size=args.vocab_size, 
                       num_layers=args.num_layers,
                       num_heads=args.num_heads, 
                       model_dim=args.model_dim,
                       max_seq_len=max(args.train_seq_len, args.val_seq_len),
                       mlp_ratio=args.mlp_ratio,
                       eigen_rank=args.eigen_rank).cuda()
print0(f'{model.get_num_params()} parameters', console=True)
print0(model)

for param in model.parameters():
    dist.broadcast(param.detach(), 0)

# Simple optimizer initialization - zero weight decay
optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr_init, betas=(0.9, 0.95), eps=1e-8, weight_decay=0.0)

# Learning rate schedule without warmup
def get_lr(step: int): # TODO add warmup for regular GPT
    # Cosine decay phase only
    cosine_decay = 0.5 * (1 + math.cos(math.pi * step / args.num_iterations))
    return max(cosine_decay, args.lr_final / args.lr_init)

# Use a more memory-efficient compilation option
model: nn.Module = torch.compile(model, dynamic=False, mode="reduce-overhead")

# Add fallback mode to handle compilation errors
import torch._dynamo
torch._dynamo.config.suppress_errors = True

########################################
#            Warmup kernels            #
########################################

# Attempt to limit memory fragmentation
if hasattr(torch.cuda, 'memory_stats'):
    print0(f"Initial GPU memory: {torch.cuda.memory_allocated() // (1024 * 1024)} MB")

# Warmup the training kernels, then re-initialize the state so we aren't cheating
warmup_steps = 10
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizer=copy.deepcopy(optimizer.state_dict())) # save the initial state
for _ in range(warmup_steps):
    inputs = targets = torch.randint(0, args.vocab_size, size=(args.train_seq_len,), device="cuda")
    model(inputs.to(torch.int32), targets).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    optimizer.step()
    model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
optimizer.load_state_dict(initial_state["optimizer"])
del initial_state

if hasattr(torch.cuda, 'memory_stats'):
    print0(f"After warmup GPU memory: {torch.cuda.memory_allocated() // (1024 * 1024)} MB")

########################################
#        Training and validation       #
########################################

def sample_from_model(model, prompt, max_new_tokens=100, temperature=0.8, top_k=200):
    """Generate text samples from the model given a prompt."""
    # We need an encoding function - assuming you'll use tiktoken or similar
    enc = tiktoken.get_encoding("gpt2")
    encode = lambda s: enc.encode(s, allowed_special={"<|endoftext|>"})
    decode = lambda l: enc.decode(l)
    
    # Encode the prompt
    input_ids = encode(prompt)
    x = torch.tensor(input_ids, dtype=torch.int32, device="cuda")
    
    # Generate
    model.eval()
    with torch.no_grad():
        y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)
    
    # Decode and return
    return decode(y.tolist())

train_loader = distributed_data_generator(args.train_files, world_size * args.train_seq_len, rank, world_size)

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)

    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        
        # Use smaller val batch for GPUs w/ at least 8GB VRAM during testing
        val_batch_size = world_size * args.val_seq_len
        # Ensure we validate on enough tokens while keeping memory usage reasonable
        val_steps = max(1, min(16, args.val_tokens // val_batch_size))
        val_tokens_used = val_batch_size * val_steps
        
        print0(f"Validating on {val_tokens_used} tokens ({val_steps} steps with {val_batch_size} batch size)")
        
        # Choose between real data loader and synthetic data loader for validation
        val_loader = distributed_data_generator(args.val_files, val_batch_size, rank, world_size)
        val_loss = 0
        with torch.no_grad():
            for i in range(val_steps):
                inputs, targets = next(val_loader)
                # Check if inputs exceed sequence length
                if inputs.size(0) > args.val_seq_len:
                    inputs = inputs[:args.val_seq_len]
                    targets = targets[:args.val_seq_len]
                val_loss += model(inputs, targets)
        val_loss /= val_steps
        del val_loader
        dist.all_reduce(val_loss, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizer=optimizer.state_dict())
            os.makedirs(f"experiments/{run_id}", exist_ok=True) # Changed from "logs/" to "experiments/"
            torch.save(log, f"experiments/{run_id}/state_step{step:06d}.pt") # Changed from "logs/" to "experiments/"
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    inputs, targets = next(train_loader)
    
    # Check if inputs exceed sequence length - can happen if the dataset has different sized examples
    if inputs.size(0) > args.train_seq_len:
        inputs = inputs[:args.train_seq_len]
        targets = targets[:args.train_seq_len]
        
    model(inputs, targets).backward()
    for param in model.parameters():
        dist.all_reduce(param.grad, op=dist.ReduceOp.AVG)
    # set optimization hyperparameters
    for group in optimizer.param_groups:
        group["lr"] = args.lr_init * get_lr(step)
    # step the optimizers
    optimizer.step()
    # Apply cosine normalization & absolute value constraints after optimization step
    model.enforce_constraints()  
    # null the gradients
    model.zero_grad(set_to_none=True)
        
    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

# Then at the end of training:
if master_process:
    # check to make sure abs val & cos norm actually worked
    # checking to make sure the absolute value-ing worked
    print0("-"*10 + " making sure assertions worked " + "-"*10, console=True)
    #print0(model.blocks[0].alpha_A.s.data[:5], console=True)
    # checking to make sure the cosine normalization worked
    print0(model.blocks[0].mlp.Wup.weight.norm(dim=1)[:5], console=True)
    print0(model.embed.weight.norm(dim=1)[:5], console=True)

    prompts = [
        "Once upon a time,",
        "The meaning of life is",
        "In the year 2026,",
        "I'm a Large Language Model (LLM), which means"
    ]
    for prompt in prompts:
        continuation = sample_from_model(model, prompt, max_new_tokens=16)
        print0(continuation, console=True)

########################################
#        HellaSwag Evaluation         #
########################################

def render_hellaswag_example(example, enc):
    """
    Given the example as a dictionary, render it as three torch tensors:
    - tokens (the tokens of context + completion, of size 4xN, as there are always 4 candidates)
    - mask (is 1 in the region of the candidate completion, where we evaluate likelihoods)
    - label (the index of the correct completion, which we hope has the highest likelihood)
    """
    ctx = example["ctx"]
    label = example["label"]
    endings = example["endings"]

    # gather up all the tokens
    ctx_tokens = enc.encode(ctx)
    tok_rows = []
    mask_rows = []
    for end in endings:
        end_tokens = enc.encode(" " + end)  # note: prepending " " because GPT-2 tokenizer
        tok_rows.append(ctx_tokens + end_tokens)
        mask_rows.append([0]*len(ctx_tokens) + [1]*len(end_tokens))

    # have to be careful during the collation because the number of tokens in each row can differ
    max_len = max(len(row) for row in tok_rows)
    tokens = torch.zeros((4, max_len), dtype=torch.int32)
    mask = torch.zeros((4, max_len), dtype=torch.int32)
    for i, (tok_row, mask_row) in enumerate(zip(tok_rows, mask_rows)):
        tokens[i, :len(tok_row)] = torch.tensor(tok_row)
        mask[i, :len(mask_row)] = torch.tensor(mask_row)

    return tokens, mask, label

def iterate_hellaswag_examples(data_path, limit=None):
    """Iterate through HellaSwag examples, with optional limit"""
    with open(data_path, "r") as f:
        for i, line in enumerate(f):
            if limit is not None and i >= limit:
                break
            example = json.loads(line)
            yield example

@torch.no_grad()
def evaluate_hellaswag(model, data_path, limit=None):
    """Evaluate model on HellaSwag"""
    print0("Starting HellaSwag evaluation...", console=True)
    
    # Add this line at the beginning of the function to disable dynamo compilation for evaluation
    torch._dynamo.config.disable = True
    
    # Set up tokenizer
    enc = tiktoken.get_encoding("gpt2")
    
    model.eval()
    
    num_correct_norm = 0
    num_correct = 0
    num_total = 0
    
    for example in iterate_hellaswag_examples(data_path, limit):
        tokens, mask, label = render_hellaswag_example(example, enc)
        tokens = tokens.to(device="cuda")
        mask = mask.to(device="cuda")

        # Process each candidate one at a time to avoid memory issues
        losses = []
        normalized_losses = []
        
        for i in range(4):  # 4 candidates per example
            # Get token sequence for this candidate
            seq = tokens[i]
            seq_mask = mask[i]
            
            # Only process up to valid tokens (not padding)
            valid_len = (seq > 0).sum().item()
            if valid_len == 0:
                continue
                
            valid_seq = seq[:valid_len]
            
            # Get logits from our model
            logits = model(valid_seq)
            if isinstance(logits, torch.Tensor):
                logits = logits[0]  # Our model returns [B, T, V] but B=1
            
            # Evaluate the autoregressive loss
            shift_logits = logits[:-1, :]
            shift_tokens = valid_seq[1:].to(torch.int64)  # Target needs to be int64
            shift_mask = seq_mask[1:valid_len]  # Shift mask to align with shifted tokens
            
            # Calculate loss for each position
            losses_per_token = F.cross_entropy(
                shift_logits, shift_tokens, reduction='none'
            )
            
            # Apply mask to focus on completion region
            masked_losses = losses_per_token * shift_mask
            
            # Calculate total and normalized loss
            total_loss = masked_losses.sum()
            completion_token_count = shift_mask.sum()
            normalized_loss = total_loss / completion_token_count if completion_token_count > 0 else float('inf')
            
            losses.append(total_loss.item())
            normalized_losses.append(normalized_loss.item())
        
        # Get predictions
        pred = torch.tensor(losses).argmin().item()
        pred_norm = torch.tensor(normalized_losses).argmin().item()
        
        # Accumulate stats
        num_total += 1
        num_correct += int(pred == label)
        num_correct_norm += int(pred_norm == label)
        
        if num_total <= 5:  # Show details for first few examples
            print0(f"---\nContext:\n {example['ctx']}\nEndings:", console=True)
            for i, end in enumerate(example["endings"]):
                print0(f"{i} (loss: {normalized_losses[i]:.4f}) {end}", console=True)
            print0(f"predicted: {pred_norm}, actual: {label}", console=True)
    
    # Calculate accuracy
    accuracy = num_correct / num_total if num_total > 0 else 0
    accuracy_norm = num_correct_norm / num_total if num_total > 0 else 0
    
    # Calculate 95% confidence intervals using Wilson score interval
    # This is more robust than normal approximation, especially for small sample sizes or extreme probabilities
    z = 1.96  # 95% confidence
    
    def wilson_conf_interval(correct, total):
        """Calculate Wilson score interval for a binary proportion"""
        if total == 0:
            return (0, 0)
        
        p = correct / total
        denominator = 1 + z**2 / total
        centre_adjusted_p = (p + z**2 / (2 * total)) / denominator
        adjusted_interval = z * ((p * (1 - p) / total + z**2 / (4 * total**2)) ** 0.5) / denominator
        
        lower = max(0, centre_adjusted_p - adjusted_interval)
        upper = min(1, centre_adjusted_p + adjusted_interval)
        
        return (lower, upper)
    
    # Get confidence intervals
    ci = wilson_conf_interval(num_correct, num_total)
    ci_norm = wilson_conf_interval(num_correct_norm, num_total)
    
    # Final results
    print0(f"HellaSwag evaluation complete - {num_total} examples", console=True)
    print0(f"Accuracy: {num_correct}/{num_total}={accuracy:.4f} [95% CI: {ci[0]:.4f}-{ci[1]:.4f}]", console=True)
    print0(f"Normalized accuracy: {num_correct_norm}/{num_total}={accuracy_norm:.4f} [95% CI: {ci_norm[0]:.4f}-{ci_norm[1]:.4f}]", console=True)

# After training and sample generations, evaluate on HellaSwag
if master_process:
    hellaswag_path = "./data/hellaswag/hellaswag_val.jsonl"  # Adjust path as needed
    
    # Check if the HellaSwag data file exists
    if os.path.exists(hellaswag_path):
        print0(f"Found HellaSwag dataset at {hellaswag_path}, running evaluation...", console=True)
        evaluate_hellaswag(model, hellaswag_path, limit=20)
    else:
        print0(f"HellaSwag dataset not found at {hellaswag_path}, skipping evaluation.", console=True)


====================================================================================================
hellaswag.py not found
====================================================================================================
====================================================================================================
Running Python 3.11.10 | packaged by conda-forge | (main, Oct 16 2024, 01:27:36) [GCC 13.3.0]
Running PyTorch 2.6.0+cu124 compiled for CUDA 12.4
Sat Mar 29 05:39:53 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.107.02             Driver Version: 550.107.02     CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3060 Ti     On  |   00000000:04:00.0 Off |                  N/A |
|  0%   47C    P2             42W /  200W |     434MiB /   8192MiB |     10%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA GeForce RTX 3060 Ti     On  |   00000000:2B:00.0 Off |                  N/A |
|  0%   45C    P2             19W /  200W |     261MiB /   8192MiB |      5%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

====================================================================================================
44720512 parameters
GPT(
  (embed): Embedding(50257, 384)
  (blocks): ModuleList(
    (0-3): 4 x Block(
      (attn): CausalSelfAttention(
        (Wq): Linear(in_features=384, out_features=384, bias=False)
        (Wk): Linear(in_features=384, out_features=384, bias=False)
        (Wv): Linear(in_features=384, out_features=384, bias=False)
        (s_qk): Scale()
        (rotary): Rotary()
        (Wo): Linear(in_features=384, out_features=384, bias=False)
      )
      (mlp): MLP(
        (Wup): Linear(in_features=384, out_features=768, bias=False)
        (Wgate): Linear(in_features=384, out_features=768, bias=False)
        (Wdown): Linear(in_features=768, out_features=384, bias=False)
        (s_u): Scale()
        (s_v): Scale()
      )
      (alpha_A): MoScale(
        (w): Linear(in_features=384, out_features=32, bias=True)
      )
      (alpha_M): MoScale(
        (w): Linear(in_features=384, out_features=32, bias=True)
      )
    )
  )
  (lm_head): Linear(in_features=384, out_features=50304, bias=True)
  (s_z): Scale()
)
Initial GPU memory: 178 MB
After warmup GPU memory: 178 MB
Validating on 262144 tokens (16 steps with 16384 batch size)
Total tokens across 1 shard(s): 100,000,000
Tokens needed for 200 iterations: 3,276,800
Training will use approximately 0.03 epochs over the data
step:0/200 val_loss:83.1763 train_time:0ms step_avg:0.15ms
Total tokens across 1 shard(s): 100,000,000
Tokens needed for 200 iterations: 3,276,800
Training will use approximately 0.03 epochs over the data
step:1/200 train_time:400ms step_avg:399.69ms
step:2/200 train_time:406ms step_avg:202.76ms
step:3/200 train_time:411ms step_avg:137.06ms
step:4/200 train_time:417ms step_avg:104.17ms
step:5/200 train_time:422ms step_avg:84.40ms
step:6/200 train_time:427ms step_avg:71.22ms
step:7/200 train_time:433ms step_avg:61.80ms
step:8/200 train_time:438ms step_avg:54.73ms
step:9/200 train_time:715ms step_avg:79.41ms
step:10/200 train_time:1030ms step_avg:102.97ms
step:11/200 train_time:1413ms step_avg:128.50ms
step:12/200 train_time:1713ms step_avg:142.78ms
step:13/200 train_time:2013ms step_avg:154.83ms
step:14/200 train_time:2314ms step_avg:165.30ms
step:15/200 train_time:2612ms step_avg:174.16ms
step:16/200 train_time:2990ms step_avg:186.89ms
step:17/200 train_time:3370ms step_avg:198.23ms
step:18/200 train_time:3759ms step_avg:208.84ms
step:19/200 train_time:4136ms step_avg:217.67ms
step:20/200 train_time:4446ms step_avg:222.29ms
step:21/200 train_time:4748ms step_avg:226.09ms
step:22/200 train_time:5068ms step_avg:230.35ms
step:23/200 train_time:5365ms step_avg:233.26ms
step:24/200 train_time:5660ms step_avg:235.83ms
step:25/200 train_time:5992ms step_avg:239.69ms
step:26/200 train_time:6283ms step_avg:241.67ms
step:27/200 train_time:6587ms step_avg:243.96ms
step:28/200 train_time:6917ms step_avg:247.04ms
step:29/200 train_time:7224ms step_avg:249.09ms
step:30/200 train_time:7541ms step_avg:251.38ms
step:31/200 train_time:7839ms step_avg:252.87ms
step:32/200 train_time:8148ms step_avg:254.62ms
step:33/200 train_time:8452ms step_avg:256.11ms
step:34/200 train_time:8772ms step_avg:257.99ms
step:35/200 train_time:9069ms step_avg:259.10ms
step:36/200 train_time:9374ms step_avg:260.38ms
step:37/200 train_time:9683ms step_avg:261.70ms
step:38/200 train_time:10001ms step_avg:263.19ms
step:39/200 train_time:10299ms step_avg:264.07ms
step:40/200 train_time:10599ms step_avg:264.98ms
step:41/200 train_time:10923ms step_avg:266.42ms
step:42/200 train_time:11279ms step_avg:268.54ms
step:43/200 train_time:11578ms step_avg:269.25ms
step:44/200 train_time:11934ms step_avg:271.24ms
step:45/200 train_time:12267ms step_avg:272.59ms
step:46/200 train_time:12563ms step_avg:273.10ms
step:47/200 train_time:12869ms step_avg:273.81ms
step:48/200 train_time:13214ms step_avg:275.30ms
step:49/200 train_time:13519ms step_avg:275.90ms
step:50/200 train_time:13856ms step_avg:277.13ms
step:51/200 train_time:14158ms step_avg:277.60ms
step:52/200 train_time:14493ms step_avg:278.70ms
step:53/200 train_time:14793ms step_avg:279.12ms
step:54/200 train_time:15100ms step_avg:279.63ms
step:55/200 train_time:15435ms step_avg:280.64ms
step:56/200 train_time:15734ms step_avg:280.96ms
step:57/200 train_time:16118ms step_avg:282.78ms
step:58/200 train_time:16506ms step_avg:284.59ms
step:59/200 train_time:16816ms step_avg:285.02ms
step:60/200 train_time:17179ms step_avg:286.32ms
step:61/200 train_time:17491ms step_avg:286.73ms
step:62/200 train_time:17789ms step_avg:286.92ms
step:63/200 train_time:18157ms step_avg:288.21ms
step:64/200 train_time:18539ms step_avg:289.68ms
step:65/200 train_time:18928ms step_avg:291.19ms
step:66/200 train_time:19238ms step_avg:291.48ms
step:67/200 train_time:19596ms step_avg:292.48ms
step:68/200 train_time:19899ms step_avg:292.63ms
step:69/200 train_time:20261ms step_avg:293.63ms
step:70/200 train_time:20561ms step_avg:293.73ms
step:71/200 train_time:20861ms step_avg:293.82ms
step:72/200 train_time:21158ms step_avg:293.86ms
step:73/200 train_time:21450ms step_avg:293.84ms
step:74/200 train_time:21790ms step_avg:294.46ms
step:75/200 train_time:22089ms step_avg:294.52ms
step:76/200 train_time:22395ms step_avg:294.67ms
step:77/200 train_time:22768ms step_avg:295.69ms
step:78/200 train_time:23070ms step_avg:295.77ms
step:79/200 train_time:23392ms step_avg:296.10ms
step:80/200 train_time:23721ms step_avg:296.52ms
step:81/200 train_time:24062ms step_avg:297.07ms
step:82/200 train_time:24379ms step_avg:297.30ms
step:83/200 train_time:24768ms step_avg:298.41ms
step:84/200 train_time:25111ms step_avg:298.95ms
step:85/200 train_time:25440ms step_avg:299.29ms
step:86/200 train_time:25776ms step_avg:299.72ms
step:87/200 train_time:26088ms step_avg:299.87ms
step:88/200 train_time:26396ms step_avg:299.95ms
step:89/200 train_time:26708ms step_avg:300.10ms
step:90/200 train_time:27026ms step_avg:300.29ms
step:91/200 train_time:27365ms step_avg:300.71ms
step:92/200 train_time:27723ms step_avg:301.34ms
step:93/200 train_time:28028ms step_avg:301.37ms
step:94/200 train_time:28324ms step_avg:301.32ms
step:95/200 train_time:28623ms step_avg:301.29ms
step:96/200 train_time:29006ms step_avg:302.15ms
step:97/200 train_time:29311ms step_avg:302.18ms
step:98/200 train_time:29629ms step_avg:302.34ms
step:99/200 train_time:29939ms step_avg:302.42ms
step:100/200 train_time:30235ms step_avg:302.35ms
Validating on 262144 tokens (16 steps with 16384 batch size)
Total tokens across 1 shard(s): 100,000,000
Tokens needed for 200 iterations: 3,276,800
Training will use approximately 0.03 epochs over the data
step:100/200 val_loss:9.9446 train_time:32521ms step_avg:325.21ms
step:101/200 train_time:32527ms step_avg:322.05ms
step:102/200 train_time:32533ms step_avg:318.95ms
step:103/200 train_time:32538ms step_avg:315.90ms
step:104/200 train_time:32543ms step_avg:312.92ms
step:105/200 train_time:32549ms step_avg:309.99ms
step:106/200 train_time:32554ms step_avg:307.11ms
step:107/200 train_time:32559ms step_avg:304.29ms
step:108/200 train_time:32904ms step_avg:304.67ms
step:109/200 train_time:33243ms step_avg:304.99ms
step:110/200 train_time:33626ms step_avg:305.69ms
step:111/200 train_time:34017ms step_avg:306.46ms
step:112/200 train_time:34324ms step_avg:306.47ms
step:113/200 train_time:34651ms step_avg:306.64ms
step:114/200 train_time:35000ms step_avg:307.01ms
step:115/200 train_time:35304ms step_avg:307.00ms
step:116/200 train_time:35601ms step_avg:306.90ms
step:117/200 train_time:35983ms step_avg:307.55ms
step:118/200 train_time:36374ms step_avg:308.26ms
step:119/200 train_time:36759ms step_avg:308.90ms
step:120/200 train_time:37071ms step_avg:308.92ms
step:121/200 train_time:37394ms step_avg:309.04ms
step:122/200 train_time:37780ms step_avg:309.67ms
step:123/200 train_time:38091ms step_avg:309.68ms
step:124/200 train_time:38397ms step_avg:309.66ms
step:125/200 train_time:38692ms step_avg:309.53ms
step:126/200 train_time:39001ms step_avg:309.54ms
step:127/200 train_time:39297ms step_avg:309.43ms
step:128/200 train_time:39616ms step_avg:309.50ms
step:129/200 train_time:39943ms step_avg:309.63ms
step:130/200 train_time:40335ms step_avg:310.27ms
step:131/200 train_time:40666ms step_avg:310.43ms
step:132/200 train_time:40970ms step_avg:310.38ms
step:133/200 train_time:41293ms step_avg:310.48ms
step:134/200 train_time:41652ms step_avg:310.84ms
step:135/200 train_time:41953ms step_avg:310.76ms
step:136/200 train_time:42254ms step_avg:310.69ms
step:137/200 train_time:42555ms step_avg:310.62ms
step:138/200 train_time:42881ms step_avg:310.73ms
step:139/200 train_time:43177ms step_avg:310.63ms
step:140/200 train_time:43510ms step_avg:310.79ms
step:141/200 train_time:43815ms step_avg:310.75ms
step:142/200 train_time:44119ms step_avg:310.70ms
step:143/200 train_time:44425ms step_avg:310.66ms
step:144/200 train_time:44815ms step_avg:311.21ms
step:145/200 train_time:45121ms step_avg:311.18ms
step:146/200 train_time:45504ms step_avg:311.67ms
step:147/200 train_time:45828ms step_avg:311.76ms
step:148/200 train_time:46128ms step_avg:311.68ms
step:149/200 train_time:46518ms step_avg:312.20ms
step:150/200 train_time:46826ms step_avg:312.17ms
step:151/200 train_time:47139ms step_avg:312.18ms
step:152/200 train_time:47440ms step_avg:312.10ms
step:153/200 train_time:47760ms step_avg:312.15ms
step:154/200 train_time:48073ms step_avg:312.16ms
step:155/200 train_time:48387ms step_avg:312.18ms
step:156/200 train_time:48687ms step_avg:312.10ms
step:157/200 train_time:48992ms step_avg:312.05ms
step:158/200 train_time:49311ms step_avg:312.10ms
step:159/200 train_time:49630ms step_avg:312.14ms
step:160/200 train_time:50020ms step_avg:312.63ms
step:161/200 train_time:50338ms step_avg:312.66ms
step:162/200 train_time:50714ms step_avg:313.05ms
step:163/200 train_time:51019ms step_avg:313.00ms
step:164/200 train_time:51320ms step_avg:312.93ms
step:165/200 train_time:51629ms step_avg:312.91ms
step:166/200 train_time:51999ms step_avg:313.25ms
step:167/200 train_time:52295ms step_avg:313.15ms
step:168/200 train_time:52609ms step_avg:313.15ms
step:169/200 train_time:52907ms step_avg:313.06ms
step:170/200 train_time:53210ms step_avg:313.00ms
step:171/200 train_time:53535ms step_avg:313.07ms
step:172/200 train_time:53874ms step_avg:313.22ms
step:173/200 train_time:54184ms step_avg:313.20ms
step:174/200 train_time:54489ms step_avg:313.15ms
step:175/200 train_time:54839ms step_avg:313.37ms
step:176/200 train_time:55144ms step_avg:313.32ms
step:177/200 train_time:55469ms step_avg:313.38ms
step:178/200 train_time:55775ms step_avg:313.34ms
step:179/200 train_time:56073ms step_avg:313.26ms
step:180/200 train_time:56378ms step_avg:313.21ms
step:181/200 train_time:56760ms step_avg:313.59ms
step:182/200 train_time:57095ms step_avg:313.71ms
step:183/200 train_time:57398ms step_avg:313.65ms
step:184/200 train_time:57696ms step_avg:313.56ms
step:185/200 train_time:57995ms step_avg:313.48ms
step:186/200 train_time:58303ms step_avg:313.46ms
step:187/200 train_time:58602ms step_avg:313.38ms
step:188/200 train_time:58922ms step_avg:313.42ms
step:189/200 train_time:59240ms step_avg:313.44ms
step:190/200 train_time:59546ms step_avg:313.40ms
step:191/200 train_time:59852ms step_avg:313.36ms
step:192/200 train_time:60234ms step_avg:313.72ms
step:193/200 train_time:60548ms step_avg:313.72ms
step:194/200 train_time:60904ms step_avg:313.94ms
step:195/200 train_time:61218ms step_avg:313.94ms
step:196/200 train_time:61513ms step_avg:313.84ms
step:197/200 train_time:61833ms step_avg:313.87ms
step:198/200 train_time:62182ms step_avg:314.05ms
step:199/200 train_time:62567ms step_avg:314.41ms
step:200/200 train_time:62873ms step_avg:314.36ms
Validating on 262144 tokens (16 steps with 16384 batch size)
Total tokens across 1 shard(s): 100,000,000
Tokens needed for 200 iterations: 3,276,800
Training will use approximately 0.03 epochs over the data
step:200/200 val_loss:9.3924 train_time:65400ms step_avg:327.00ms
peak memory allocated: 6594 MiB reserved: 6818 MiB
---------- making sure assertions worked ----------
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0',
       grad_fn=<SliceBackward0>)
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0',
       grad_fn=<SliceBackward0>)
Once upon a time, please over place still will people used some they D see more been with those especially
The meaning of life is when two same which a recent more which during then made UItIt complete."
In the year 2026, more as: development from used on best a could very less making  the full
I'm a Large Language Model (LLM), which means all many There has other development2 day would, at who that)It
Found HellaSwag dataset at ./data/hellaswag/hellaswag_val.jsonl, running evaluation...
Starting HellaSwag evaluation...
---
Context:
 A man is sitting on a roof. he
Endings:
0 (loss: 9.3678) is using wrap to wrap a pair of skis.
1 (loss: 9.7672) is ripping level tiles off.
2 (loss: 9.5208) is holding a rubik's cube.
3 (loss: 9.5345) starts pulling up roofing on a roof.
predicted: 0, actual: 3
---
Context:
 A lady walks to a barbell. She bends down and grabs the pole. the lady
Endings:
0 (loss: 9.5633) swings and lands in her arms.
1 (loss: 9.8583) pulls the barbell forward.
2 (loss: 9.7421) pulls a rope attached to the barbell.
3 (loss: 9.5318) stands and lifts the weight over her head.
predicted: 3, actual: 3
---
Context:
 Two women in a child are shown in a canoe while a man pulls the canoe while standing in the water, with other individuals visible in the background. the child and a different man
Endings:
0 (loss: 9.3528) are then shown paddling down a river in a boat while a woman talks.
1 (loss: 9.1554) are driving the canoe, they go down the river flowing side to side.
2 (loss: 9.5284) sit in a canoe while the man paddles.
3 (loss: 9.3501) walking go down the rapids, while the man in his helicopter almost falls and goes out of canoehood.
predicted: 1, actual: 2
---
Context:
 A boy is running down a track. the boy
Endings:
0 (loss: 9.0852) runs into a car.
1 (loss: 9.1708) gets in a mat.
2 (loss: 9.3049) lifts his body above the height of a pole.
3 (loss: 9.2024) stands on his hands and springs.
predicted: 0, actual: 2
---
Context:
 The boy lifts his body above the height of a pole. The boy lands on his back on to a red mat. the boy
Endings:
0 (loss: 9.1512) turns his body around on the mat.
1 (loss: 9.1387) gets up from the mat.
2 (loss: 9.3608) continues to lift his body over the pole.
3 (loss: 9.3579) wiggles out of the mat.
predicted: 1, actual: 1
HellaSwag evaluation complete - 20 examples
Accuracy: 5/20=0.2500 [95% CI: 0.1119-0.4687]
Normalized accuracy: 4/20=0.2000 [95% CI: 0.0807-0.4160]
